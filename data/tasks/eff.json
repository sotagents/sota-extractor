[
  {
    "task": "Music Autotagging",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Precision of Instrumentals detection reached when tested on SATIN (Bayle et al. 2017)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Ghosal et al.",
              "paper_title": "A hierarchical approach for speech-instrumental-song classification | SpringerLink",
              "paper_url": "https://link.springer.com/article/10.1186/2193-1801-2-526",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 17.3
              }
            },
            {
              "model_name": "VQMM",
              "paper_title": "On Evaluation Validity in Music Autotagging",
              "paper_url": "https://arxiv.org/abs/1410.0001",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 29.8
              }
            },
            {
              "model_name": "SVMBFF",
              "paper_title": "On Evaluation Validity in Music Autotagging",
              "paper_url": "https://arxiv.org/abs/1410.0001",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 12.5
              }
            },
            {
              "model_name": "Bayle et al.",
              "paper_title": "Revisiting Autotagging Toward Faultless Instrumental Playlists Generation",
              "paper_url": "https://arxiv.org/abs/1706.07613",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Abstract Strategy Games",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Abstract Strategy Games with Rule Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Constrained Problem Solving",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Restricted Reproduction",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Arcade Game Transfer Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Privacy Preserving Machine Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Video Activity Recognition",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Reward Hacking Avoidance",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Image Generation",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Generative models of CIFAR-10 images",
        "description": "",
        "sota": {
          "metrics": [
            "Model Entropy"
          ],
          "rows": [
            {
              "model_name": "NICE",
              "paper_title": "NICE: Non-linear Independent Components Estimation",
              "paper_url": "https://arxiv.org/abs/1410.8516",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 4.48
              }
            },
            {
              "model_name": "DRAW",
              "paper_title": "DRAW: A Recurrent Neural Network For Image Generation",
              "paper_url": "https://arxiv.org/abs/1502.04623",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 4.13
              }
            },
            {
              "model_name": "PixelRNN",
              "paper_title": "Density estimation using Real NVP",
              "paper_url": "https://arxiv.org/abs/1605.08803",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 3.0
              }
            },
            {
              "model_name": "Real NVP",
              "paper_title": "Density estimation using Real NVP",
              "paper_url": "https://arxiv.org/abs/1605.08803",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 3.49
              }
            },
            {
              "model_name": "VAE with IAF",
              "paper_title": "Improved Variational Inference with Inverse Autoregressive Flow",
              "paper_url": "https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 3.11
              }
            },
            {
              "model_name": "PixelCNN++",
              "paper_title": "Forum | OpenReview",
              "paper_url": "https://openreview.net/forum?id=BJrFC6ceg",
              "paper_date": null,
              "code_links": [
                {
                  "title": "Replicated",
                  "url": "https://github.com/openai/pixel-cnn"
                }
              ],
              "model_links": [],
              "metrics": {
                "Model Entropy": 2.92
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Classification Under Uncertainty",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Automated Security",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Scalable Supervised Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Pedestrian Detection",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Automated Circuit Design",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "One-Shot Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Question Answering",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "bAbi 20 QA (10k training examples)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "MemNN-AM+NG+NL (1k + strong supervision)",
              "paper_title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
              "paper_url": "https://arxiv.org/abs/1502.05698v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.3
              }
            },
            {
              "model_name": "MemN2N-PE+LS+RN",
              "paper_title": "End-To-End Memory Networks",
              "paper_url": "https://arxiv.org/abs/1503.08895",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.4
              }
            },
            {
              "model_name": "DNC",
              "paper_title": null,
              "paper_url": "https://www.gwern.net/docs/2016-graves.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 96.2
              }
            },
            {
              "model_name": "DMN+",
              "paper_title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes",
              "paper_url": "https://arxiv.org/abs/1607.00036",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 97.2
              }
            },
            {
              "model_name": "SDNC",
              "paper_title": "Query-Reduction Networks for Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.04582v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 97.1
              }
            },
            {
              "model_name": "QRN",
              "paper_title": "Query-Reduction Networks for Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.04582v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 99.7
              }
            },
            {
              "model_name": "EntNet",
              "paper_title": "Tracking the World State with Recurrent Entity Networks",
              "paper_url": "https://arxiv.org/abs/1612.03969",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 99.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "bAbi 20 QA (1k training examples)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "MemN2N-PE+LS+RN",
              "paper_title": "End-To-End Memory Networks",
              "paper_url": "https://arxiv.org/abs/1503.08895",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 86.1
              }
            },
            {
              "model_name": "DMN",
              "paper_title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
              "paper_url": "https://arxiv.org/abs/1506.07285",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.6
              }
            },
            {
              "model_name": "QRN",
              "paper_title": "Query-Reduction Networks for Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.04582v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.1
              }
            },
            {
              "model_name": "DMN+",
              "paper_title": "Query-Reduction Networks for Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.04582v4",
              "paper_date": null,
              "code_links": [
                {
                  "title": "Replicated",
                  "url": "https://github.com/therne/dmn-tensorflow"
                }
              ],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.8
              }
            },
            {
              "model_name": "EntNet",
              "paper_title": "Tracking the World State with Recurrent Entity Networks",
              "paper_url": "https://arxiv.org/abs/1612.03969",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 89.1
              }
            },
            {
              "model_name": "GA+MAGE (16)",
              "paper_title": "Linguistic Knowledge as Memory for Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1703.02620v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Reading comprehension MCTest-160-all",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "SW+D+RTE",
              "paper_title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.16
              }
            },
            {
              "model_name": "Wang-et-al",
              "paper_title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
              "paper_url": "https://arxiv.org/abs/1603.08884",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.27
              }
            },
            {
              "model_name": "Narasimhan-model3",
              "paper_title": "Machine Comprehension with Discourse Relations",
              "paper_url": "https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 73.27
              }
            },
            {
              "model_name": "Parallel-Hierarchical",
              "paper_title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
              "paper_url": "https://arxiv.org/abs/1603.08884",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.58
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Reading comprehension MCTest-500-all",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "SW+D+RTE",
              "paper_title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.33
              }
            },
            {
              "model_name": "Wang-et-al",
              "paper_title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
              "paper_url": "https://arxiv.org/abs/1603.08884",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.94
              }
            },
            {
              "model_name": "LSSVM",
              "paper_title": "Learning Answer-Entailing Structures for Machine Comprehension",
              "paper_url": "https://pdfs.semanticscholar.org/f26e/088bc4659a9b7fce28b6604d26de779bcf93.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.83
              }
            },
            {
              "model_name": "Narasimhan-model3",
              "paper_title": "Machine Comprehension with Discourse Relations",
              "paper_url": "https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.75
              }
            },
            {
              "model_name": "Parallel-Hierarchical",
              "paper_title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
              "paper_url": "https://arxiv.org/abs/1603.08884",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "bAbi Children's Book comprehension CBtest NE",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "AS reader (greedy)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.0
              }
            },
            {
              "model_name": "AS reader (avg)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.6
              }
            },
            {
              "model_name": "GA reader",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.9
              }
            },
            {
              "model_name": "AIA",
              "paper_title": "Iterative Alternating Neural Attention for Machine Reading",
              "paper_url": "https://arxiv.org/abs/1606.02245v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.0
              }
            },
            {
              "model_name": "AIA",
              "paper_title": "Iterative Alternating Neural Attention for Machine Reading",
              "paper_url": "https://arxiv.org/abs/1606.02245v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.0
              }
            },
            {
              "model_name": "EpiReader",
              "paper_title": "Natural Language Comprehension with the EpiReader",
              "paper_url": "https://arxiv.org/abs/1606.02270",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.7
              }
            },
            {
              "model_name": "CAS reader",
              "paper_title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.02250v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.2
              }
            },
            {
              "model_name": "AoA reader",
              "paper_title": "Attention-over-Attention Neural Networks for Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.04423",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.0
              }
            },
            {
              "model_name": "GA +feature, fix L(w)",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.9
              }
            },
            {
              "model_name": "NSE",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 73.2
              }
            },
            {
              "model_name": "DIM Reader",
              "paper_title": "DIM Reader: Dual Interaction Mode for Machine Comprehension",
              "paper_url": "http://www.cips-cl.org/static/anthology/CCL-2017/CCL-17-075.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.2
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "bAbi Children's Book comprehension CBtest CN",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "AS reader (avg)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.9
              }
            },
            {
              "model_name": "AS reader (greedy)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.5
              }
            },
            {
              "model_name": "GA reader",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.4
              }
            },
            {
              "model_name": "EpiReader",
              "paper_title": "Natural Language Comprehension with the EpiReader",
              "paper_url": "https://arxiv.org/abs/1606.02270",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.4
              }
            },
            {
              "model_name": "CAS reader",
              "paper_title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.02250v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 65.7
              }
            },
            {
              "model_name": "AoA reader",
              "paper_title": "Attention-over-Attention Neural Networks for Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.04423",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.4
              }
            },
            {
              "model_name": "NSE",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.9
              }
            },
            {
              "model_name": "GA +feature, fix L(w)",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.7
              }
            },
            {
              "model_name": "DIM Reader",
              "paper_title": "DIM Reader: Dual Interaction Mode for Machine Comprehension",
              "paper_url": "http://www.cips-cl.org/static/anthology/CCL-2017/CCL-17-075.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "CNN Comprehension test",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Impatient reader",
              "paper_title": "Teaching Machines to Read and Comprehend",
              "paper_url": "https://arxiv.org/abs/1506.03340",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.8
              }
            },
            {
              "model_name": "Attentive reader",
              "paper_title": "Teaching Machines to Read and Comprehend",
              "paper_url": "https://arxiv.org/abs/1506.03340",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.0
              }
            },
            {
              "model_name": "AS reader (avg)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.4
              }
            },
            {
              "model_name": "AS reader (greedy)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.8
              }
            },
            {
              "model_name": "GA reader",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.4
              }
            },
            {
              "model_name": "AIA",
              "paper_title": "Iterative Alternating Neural Attention for Machine Reading",
              "paper_url": "https://arxiv.org/abs/1606.02245v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.7
              }
            },
            {
              "model_name": "EpiReader",
              "paper_title": "Natural Language Comprehension with the EpiReader",
              "paper_url": "https://arxiv.org/abs/1606.02270",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.0
              }
            },
            {
              "model_name": "CAS reader",
              "paper_title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.02250v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.0
              }
            },
            {
              "model_name": "AoA reader",
              "paper_title": "Attention-over-Attention Neural Networks for Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1607.04423",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.4
              }
            },
            {
              "model_name": "Attentive+relabling+ensemble",
              "paper_title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
              "paper_url": "https://arxiv.org/abs/1606.02858",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.6
              }
            },
            {
              "model_name": "ReasoNet",
              "paper_title": "ReasoNet: Learning to Stop Reading in Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1609.05284v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.7
              }
            },
            {
              "model_name": "AIA",
              "paper_title": "Iterative Alternating Neural Attention for Machine Reading",
              "paper_url": "https://arxiv.org/abs/1606.02245v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.1
              }
            },
            {
              "model_name": "GA update L(w)",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.9
              }
            },
            {
              "model_name": "GA+MAGE (32)",
              "paper_title": "Linguistic Knowledge as Memory for Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1703.02620v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.6
              }
            },
            {
              "model_name": "DIM Reader",
              "paper_title": "DIM Reader: Dual Interaction Mode for Machine Comprehension",
              "paper_url": "http://www.cips-cl.org/static/anthology/CCL-2017/CCL-17-075.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.4
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Daily Mail Comprehension test",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Attentive reader",
              "paper_title": "Teaching Machines to Read and Comprehend",
              "paper_url": "https://arxiv.org/abs/1506.03340",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.0
              }
            },
            {
              "model_name": "Impatient reader",
              "paper_title": "Teaching Machines to Read and Comprehend",
              "paper_url": "https://arxiv.org/abs/1506.03340",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.0
              }
            },
            {
              "model_name": "AS reader (greedy)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.7
              }
            },
            {
              "model_name": "AS reader (avg)",
              "paper_title": "Text Understanding with the Attention Sum Reader Network",
              "paper_url": "https://arxiv.org/abs/1603.01547v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.1
              }
            },
            {
              "model_name": "GA reader",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.1
              }
            },
            {
              "model_name": "Attentive+relabling+ensemble",
              "paper_title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
              "paper_url": "https://arxiv.org/abs/1606.02858",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 79.2
              }
            },
            {
              "model_name": "ReasoNet",
              "paper_title": "ReasoNet: Learning to Stop Reading in Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1609.05284v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.6
              }
            },
            {
              "model_name": "GA update L(w)",
              "paper_title": "Gated-Attention Readers for Text Comprehension",
              "paper_url": "https://arxiv.org/abs/1606.01549v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.9
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Stanford Question Answering Dataset EM test",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "Dynamic Coattention Networks (ensemble)",
              "paper_title": "Dynamic Coattention Networks For Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01604v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 71.625
              }
            },
            {
              "model_name": "Dynamic Coattention Networks (single model)",
              "paper_title": "Dynamic Coattention Networks For Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01604v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 66.233
              }
            },
            {
              "model_name": "Match-LSTM+Ans-Ptr",
              "paper_title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
              "paper_url": "https://arxiv.org/abs/1608.07905v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 67.901
              }
            },
            {
              "model_name": "BiDAF (single model)",
              "paper_title": "Bidirectional Attention Flow for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1611.01603",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 68.478
              }
            },
            {
              "model_name": "MPM (ensemble)",
              "paper_title": "Multi-Perspective Context Matching for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1612.04211",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.765
              }
            },
            {
              "model_name": "MPM (single model)",
              "paper_title": "Multi-Perspective Context Matching for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1612.04211",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.387
              }
            },
            {
              "model_name": "FastQAExt",
              "paper_title": "Making Neural QA as Simple as Possible but not Simpler",
              "paper_url": "https://arxiv.org/abs/1703.04816",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.849
              }
            },
            {
              "model_name": "FastQA",
              "paper_title": "Making Neural QA as Simple as Possible but not Simpler",
              "paper_url": "https://arxiv.org/abs/1703.04816",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 68.436
              }
            },
            {
              "model_name": "BiDAF (ensemble)",
              "paper_title": "Bidirectional Attention Flow for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1611.01603",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.744
              }
            },
            {
              "model_name": "r-net (ensemble)",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 76.922
              }
            },
            {
              "model_name": "r-net (single model)",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 74.614
              }
            },
            {
              "model_name": "Document Reader (single model)",
              "paper_title": "Reading Wikipedia to Answer Open-Domain Questions",
              "paper_url": "https://arxiv.org/abs/1704.00051",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.733
              }
            },
            {
              "model_name": "SEDT+BiDAF (ensemble)",
              "paper_title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1703.00572",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.723
              }
            },
            {
              "model_name": "SEDT+BiDAF (single model)",
              "paper_title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1703.00572",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 68.478
              }
            },
            {
              "model_name": "Ruminating Reader (single model)",
              "paper_title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention",
              "paper_url": "https://arxiv.org/abs/1704.07415",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.639
              }
            },
            {
              "model_name": "Mnemonic reader (ensemble)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.754
              }
            },
            {
              "model_name": "Mnemonic reader (single model)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 69.863
              }
            },
            {
              "model_name": "jNet (ensemble)",
              "paper_title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
              "paper_url": "https://arxiv.org/abs/1703.04617",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.01
              }
            },
            {
              "model_name": "RaSoR (single model)",
              "paper_title": "Learning Recurrent Span Representations for Extractive Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01436",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.849
              }
            },
            {
              "model_name": "jNet (single model)",
              "paper_title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
              "paper_url": "https://arxiv.org/abs/1703.04617",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.607
              }
            },
            {
              "model_name": "ReasoNet ensemble",
              "paper_title": "ReasoNet: Learning to Stop Reading in Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1609.05284v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.4
              }
            },
            {
              "model_name": "MEMEN",
              "paper_title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1707.09098v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 75.37
              }
            },
            {
              "model_name": "DCN+ (ensemble)",
              "paper_title": "The Stanford Question Answering Dataset",
              "paper_url": "https://rajpurkar.github.io/SQuAD-explorer/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 78.706
              }
            },
            {
              "model_name": "RMR (ensemble)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.678
              }
            },
            {
              "model_name": "AIR-FusionNet (ensemble)",
              "paper_title": "The Stanford Question Answering Dataset",
              "paper_url": "https://rajpurkar.github.io/SQuAD-explorer/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 78.842
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Stanford Question Answering Dataset F1 test",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "Dynamic Coattention Networks (ensemble)",
              "paper_title": "Dynamic Coattention Networks For Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01604v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 80.383
              }
            },
            {
              "model_name": "Dynamic Coattention Networks (single model)",
              "paper_title": "Dynamic Coattention Networks For Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01604v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 75.896
              }
            },
            {
              "model_name": "Match-LSTM+Ans-Ptr",
              "paper_title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
              "paper_url": "https://arxiv.org/abs/1608.07905v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.022
              }
            },
            {
              "model_name": "BiDAF (single model)",
              "paper_title": "Bidirectional Attention Flow for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1611.01603",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.971
              }
            },
            {
              "model_name": "MPM (ensemble)",
              "paper_title": "Multi-Perspective Context Matching for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1612.04211",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.257
              }
            },
            {
              "model_name": "MPM (single model)",
              "paper_title": "Multi-Perspective Context Matching for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1612.04211",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 78.784
              }
            },
            {
              "model_name": "FastQAExt",
              "paper_title": "Making Neural QA as Simple as Possible but not Simpler",
              "paper_url": "https://arxiv.org/abs/1703.04816",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 78.857
              }
            },
            {
              "model_name": "FastQA",
              "paper_title": "Making Neural QA as Simple as Possible but not Simpler",
              "paper_url": "https://arxiv.org/abs/1703.04816",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.07
              }
            },
            {
              "model_name": "BiDAF (ensemble)",
              "paper_title": "Bidirectional Attention Flow for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1611.01603",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.525
              }
            },
            {
              "model_name": "r-net (ensemble)",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 84.006
              }
            },
            {
              "model_name": "r-net (single model)",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 82.458
              }
            },
            {
              "model_name": "Document Reader (single model)",
              "paper_title": "Reading Wikipedia to Answer Open-Domain Questions",
              "paper_url": "https://arxiv.org/abs/1704.00051",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 79.353
              }
            },
            {
              "model_name": "SEDT+BiDAF (ensemble)",
              "paper_title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1703.00572",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.53
              }
            },
            {
              "model_name": "SEDT+BiDAF (single model)",
              "paper_title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1703.00572",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.971
              }
            },
            {
              "model_name": "Ruminating Reader (single model)",
              "paper_title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention",
              "paper_url": "https://arxiv.org/abs/1704.07415",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 79.821
              }
            },
            {
              "model_name": "Mnemonic reader (ensemble)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.863
              }
            },
            {
              "model_name": "Mnemonic reader (single model)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 79.207
              }
            },
            {
              "model_name": "jNet (ensemble)",
              "paper_title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
              "paper_url": "https://arxiv.org/abs/1703.04617",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.517
              }
            },
            {
              "model_name": "jNet (single model)",
              "paper_title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
              "paper_url": "https://arxiv.org/abs/1703.04617",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 79.456
              }
            },
            {
              "model_name": "RaSoR (single model)",
              "paper_title": "Learning Recurrent Span Representations for Extractive Question Answering",
              "paper_url": "https://arxiv.org/abs/1611.01436",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 78.741
              }
            },
            {
              "model_name": "ReasoNet ensemble",
              "paper_title": "ReasoNet: Learning to Stop Reading in Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1609.05284v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 82.9
              }
            },
            {
              "model_name": "MEMEN",
              "paper_title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1707.09098v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 82.66
              }
            },
            {
              "model_name": "DCN+ (ensemble)",
              "paper_title": "The Stanford Question Answering Dataset",
              "paper_url": "https://rajpurkar.github.io/SQuAD-explorer/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 85.619
              }
            },
            {
              "model_name": "RMR (ensemble)",
              "paper_title": "Mnemonic Reader for Machine Comprehension",
              "paper_url": "https://arxiv.org/abs/1705.02798",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 84.888
              }
            },
            {
              "model_name": "AIR-FusionNet (ensemble)",
              "paper_title": "The Stanford Question Answering Dataset",
              "paper_url": "https://rajpurkar.github.io/SQuAD-explorer/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 85.936
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Mathematical Proofs",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Language Games",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Spoken Language Games",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Catastrophic Forgetting Avoidance",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Omitted-Variable Bias Correction",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Artificial General Intelligence",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Code Generation",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Machine Translation",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "news-test-2014 En-Fr BLEU",
        "description": "",
        "sota": {
          "metrics": [
            "BLEU score"
          ],
          "rows": [
            {
              "model_name": "PBMT",
              "paper_title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14",
              "paper_url": "http://www.anthology.aclweb.org/W/W14/W14-33.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 37
              }
            },
            {
              "model_name": "RNN-search50*",
              "paper_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
              "paper_url": "https://arxiv.org/abs/1409.0473",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 36.15
              }
            },
            {
              "model_name": "SMT+LSTM5",
              "paper_title": "Sequence to Sequence Learning with Neural Networks",
              "paper_url": "https://arxiv.org/abs/1409.3215v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 36.5
              }
            },
            {
              "model_name": "LSTM",
              "paper_title": "Sequence to Sequence Learning with Neural Networks",
              "paper_url": "https://arxiv.org/abs/1409.3215v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 34.81
              }
            },
            {
              "model_name": "LSTM6 + PosUnk",
              "paper_title": "Addressing the Rare Word Problem in Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1410.8206",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 37.5
              }
            },
            {
              "model_name": "Deep-Att + PosUnk",
              "paper_title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1606.04199",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 39.2
              }
            },
            {
              "model_name": "GNMT+RL",
              "paper_title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
              "paper_url": "https://arxiv.org/abs/1609.08144",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 39.92
              }
            },
            {
              "model_name": "MoE 2048",
              "paper_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
              "paper_url": "https://arxiv.org/abs/1701.06538",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 40.56
              }
            },
            {
              "model_name": "ConvS2S (ensemble)",
              "paper_title": "Convolutional Sequence to Sequence Learning",
              "paper_url": "https://arxiv.org/abs/1705.03122v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 41.29
              }
            },
            {
              "model_name": "ConvS2S",
              "paper_title": "Convolutional Sequence to Sequence Learning",
              "paper_url": "https://arxiv.org/abs/1705.03122v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 40.46
              }
            },
            {
              "model_name": "Transformer (big)",
              "paper_title": "Attention Is All You Need",
              "paper_url": "https://arxiv.org/abs/1706.03762",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 41
              }
            },
            {
              "model_name": "Transformer",
              "paper_title": "Attention Is All You Need",
              "paper_url": "https://arxiv.org/abs/1706.03762",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 38.1
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "news-test-2014 En-De BLEU",
        "description": "",
        "sota": {
          "metrics": [
            "BLEU score"
          ],
          "rows": [
            {
              "model_name": "PBMT",
              "paper_title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14",
              "paper_url": "http://www.anthology.aclweb.org/W/W14/W14-33.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 20.7
              }
            },
            {
              "model_name": "NSE-NSE",
              "paper_title": "Neural Semantic Encoders",
              "paper_url": "https://arxiv.org/abs/1607.04315v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 17.93
              }
            },
            {
              "model_name": "Deep-Att",
              "paper_title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1606.04199",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 20.7
              }
            },
            {
              "model_name": "GNMT+RL",
              "paper_title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
              "paper_url": "https://arxiv.org/abs/1609.08144",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 26.3
              }
            },
            {
              "model_name": "ByteNet",
              "paper_title": "Neural Machine Translation in Linear Time",
              "paper_url": "https://arxiv.org/abs/1610.10099",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 23.75
              }
            },
            {
              "model_name": "MoE 2048",
              "paper_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
              "paper_url": "https://arxiv.org/abs/1701.06538",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 26.03
              }
            },
            {
              "model_name": "ConvS2S (ensemble)",
              "paper_title": "Convolutional Sequence to Sequence Learning",
              "paper_url": "https://arxiv.org/abs/1705.03122v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 26.36
              }
            },
            {
              "model_name": "ConvS2S",
              "paper_title": "Convolutional Sequence to Sequence Learning",
              "paper_url": "https://arxiv.org/abs/1705.03122v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 25.16
              }
            },
            {
              "model_name": "SliceNet",
              "paper_title": "Depthwise Separable Convolutions for Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1706.03059",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 26.1
              }
            },
            {
              "model_name": "Transformer (big)",
              "paper_title": "Attention Is All You Need",
              "paper_url": "https://arxiv.org/abs/1706.03762",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 28.4
              }
            },
            {
              "model_name": "Transformer",
              "paper_title": "Attention Is All You Need",
              "paper_url": "https://arxiv.org/abs/1706.03762",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 27.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "news-test-2015 En-De BLEU",
        "description": "",
        "sota": {
          "metrics": [
            "BLEU score"
          ],
          "rows": [
            {
              "model_name": "S2Tree+5gram NPLM",
              "paper_title": "Edinburgh's Syntax-Based Systems at WMT 2015",
              "paper_url": "http://aclweb.org/anthology/W15-3024.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 24.1
              }
            },
            {
              "model_name": "Enc-Dec Att (char)",
              "paper_title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1603.06147v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 23.45
              }
            },
            {
              "model_name": "Enc-Dec Att (BPE)",
              "paper_title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
              "paper_url": "https://arxiv.org/abs/1603.06147v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 21.72
              }
            },
            {
              "model_name": "ByteNet",
              "paper_title": "Neural Machine Translation in Linear Time",
              "paper_url": "https://arxiv.org/abs/1610.10099",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 26.26
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "news-test-2016 En-Ro BLEU",
        "description": "",
        "sota": {
          "metrics": [
            "BLEU score"
          ],
          "rows": [
            {
              "model_name": "GRU BPE90k",
              "paper_title": "The QT21/HimL Combined Machine Translation System",
              "paper_url": "http://www.statmt.org/wmt16/pdf/W16-2320.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 28.9
              }
            },
            {
              "model_name": "ConvS2S BPE40k",
              "paper_title": "Convolutional Sequence to Sequence Learning",
              "paper_url": "https://arxiv.org/abs/1705.03122v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "BLEU score": 29.88
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Privacy Fairness",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Safe Exploration",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Speech Recognition",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Word error rate on Switchboard trained against the Hub5'00 dataset",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "CD-DNN",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 16.1
              }
            },
            {
              "model_name": "DNN-HMM",
              "paper_title": null,
              "paper_url": "https://pdfs.semanticscholar.org/ce25/00257fda92338ec0a117bea1dbc0381d7c73.pdf?_ga=1.195375081.452266805.1483390947",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 18.5
              }
            },
            {
              "model_name": "HMM-DNN +sMBR",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.6
              }
            },
            {
              "model_name": "CNN",
              "paper_title": null,
              "paper_url": "http://www.cs.toronto.edu/~asamir/papers/icassp13_cnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 11.5
              }
            },
            {
              "model_name": "DNN MMI",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.9
              }
            },
            {
              "model_name": "DNN MPE",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.9
              }
            },
            {
              "model_name": "DNN BMMI",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.9
              }
            },
            {
              "model_name": "DNN sMBR",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.6
              }
            },
            {
              "model_name": "DNN + Dropout",
              "paper_title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1406.7806v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 15
              }
            },
            {
              "model_name": "DNN",
              "paper_title": "Increasing Deep Neural Network Acoustic Model Size for Large Vocabulary Continuous Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1406.7806v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 16
              }
            },
            {
              "model_name": "CNN on MFSC/fbanks + 1 non-conv layer for FMLLR/I-Vectors concatenated in a DNN",
              "paper_title": null,
              "paper_url": "http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202014/papers/p5609-soltau.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 10.4
              }
            },
            {
              "model_name": "Deep Speech",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 20
              }
            },
            {
              "model_name": "Deep Speech + FSH",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.6
              }
            },
            {
              "model_name": "CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.6
              }
            },
            {
              "model_name": "IBM 2015",
              "paper_title": "The IBM 2015 English Conversational Telephone Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1505.05899",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 8.0
              }
            },
            {
              "model_name": "HMM-TDNN + pNorm + speed up/down speech",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_interspeech_augmentation.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.9
              }
            },
            {
              "model_name": "HMM-TDNN + iVectors",
              "paper_title": null,
              "paper_url": "http://speak.clsp.jhu.edu/uploads/publications/papers/1048_pdf.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 11
              }
            },
            {
              "model_name": "Deep CNN (10 conv, 4 FC layers), multi-scale feature maps",
              "paper_title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR",
              "paper_url": "https://arxiv.org/abs/1509.08967v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.2
              }
            },
            {
              "model_name": "IBM 2016",
              "paper_title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1604.08242v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.9
              }
            },
            {
              "model_name": "RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + \"model M\" + NNLM language model",
              "paper_title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1604.08242v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.6
              }
            },
            {
              "model_name": "HMM-TDNN trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher (10% / 15.1% respectively trained on SWBD only)",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 9.2
              }
            },
            {
              "model_name": "HMM-BLSTM trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 8.5
              }
            },
            {
              "model_name": "VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast",
              "paper_title": "The Microsoft 2016 Conversational Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1609.03528v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.3
              }
            },
            {
              "model_name": "CNN-LSTM",
              "paper_title": "Achieving Human Parity in Conversational Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1610.05256",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.6
              }
            },
            {
              "model_name": "Microsoft 2016b",
              "paper_title": "Achieving Human Parity in Conversational Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1610.05256",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.8
              }
            },
            {
              "model_name": "RNNLM",
              "paper_title": "The Microsoft 2016 Conversational Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1609.03528",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.9
              }
            },
            {
              "model_name": "Microsoft 2016",
              "paper_title": "The Microsoft 2016 Conversational Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1609.03528",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.2
              }
            },
            {
              "model_name": "ResNet + BiLSTMs acoustic model, with 40d FMLLR + i-Vector inputs, trained on SWB+Fisher+CH, n-gram + model-M + LSTM + Strided ( trous) convs-based LM trained on Switchboard+Fisher+Gigaword+Broadcast",
              "paper_title": "English Conversational Telephone Speech Recognition by Humans and Machines",
              "paper_url": "https://arxiv.org/abs/1703.02136",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "librispeech WER testclean",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "HMM-(SAT)GMM",
              "paper_title": "Kaldi ASR",
              "paper_url": "http://kaldi-asr.org/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 8.01
              }
            },
            {
              "model_name": "HMM-DNN + pNorm*",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_icassp_librispeech.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.51
              }
            },
            {
              "model_name": "HMM-TDNN + iVectors",
              "paper_title": null,
              "paper_url": "http://speak.clsp.jhu.edu/uploads/publications/papers/1048_pdf.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 4.83
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters trained on 11940h",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.33
              }
            },
            {
              "model_name": "HMM-TDNN trained with MMI + data augmentation (speed) + iVectors + 3 regularizations",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 4.28
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "librispeech WER testother",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "HMM-(SAT)GMM",
              "paper_title": "Kaldi ASR",
              "paper_url": "http://kaldi-asr.org/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 22.49
              }
            },
            {
              "model_name": "HMM-DNN + pNorm*",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_icassp_librispeech.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 13.97
              }
            },
            {
              "model_name": "TDNN + pNorm + speed up/down speech",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_interspeech_augmentation.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.51
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters trained on 11940h",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 13.25
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "wsj WER eval92",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "CNN over RAW speech (wav)",
              "paper_title": null,
              "paper_url": "http://infoscience.epfl.ch/record/203464/files/Palaz_Idiap-RR-18-2014.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.6
              }
            },
            {
              "model_name": "TC-DNN-BLSTM-DNN",
              "paper_title": "Deep Recurrent Neural Networks for Acoustic Modelling",
              "paper_url": "https://arxiv.org/abs/1504.01482v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.47
              }
            },
            {
              "model_name": "test-set on open vocabulary (i.e. harder), model = HMM-DNN + pNorm*",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_icassp_librispeech.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.63
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.6
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "wsj WER eval93",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "test-set on open vocabulary (i.e. harder), model = HMM-DNN + pNorm*",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_icassp_librispeech.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 5.66
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 4.98
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "swb_hub_500 WER fullSWBCH",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "HMM-DNN +sMBR",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2013_interspeech_dnn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 18.4
              }
            },
            {
              "model_name": "DNN + Dropout",
              "paper_title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1406.7806v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 19.1
              }
            },
            {
              "model_name": "CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 16
              }
            },
            {
              "model_name": "HMM-TDNN + pNorm + speed up/down speech",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2015_interspeech_augmentation.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 19.3
              }
            },
            {
              "model_name": "HMM-TDNN + iVectors",
              "paper_title": null,
              "paper_url": "http://speak.clsp.jhu.edu/uploads/publications/papers/1048_pdf.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 17.1
              }
            },
            {
              "model_name": "RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + \"model M\" + NNLM language model",
              "paper_title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1604.08242v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 12.2
              }
            },
            {
              "model_name": "HMM-TDNN trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher (10% / 15.1% respectively trained on SWBD only)",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 13.3
              }
            },
            {
              "model_name": "HMM-BLSTM trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 13
              }
            },
            {
              "model_name": "VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast",
              "paper_title": "The Microsoft 2016 Conversational Speech Recognition System",
              "paper_url": "https://arxiv.org/abs/1609.03528v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 11.9
              }
            },
            {
              "model_name": "ResNet + BiLSTMs acoustic model, with 40d FMLLR + i-Vector inputs, trained on SWB+Fisher+CH, n-gram + model-M + LSTM + Strided ( trous) convs-based LM trained on Switchboard+Fisher+Gigaword+Broadcast",
              "paper_title": "English Conversational Telephone Speech Recognition by Humans and Machines",
              "paper_url": "https://arxiv.org/abs/1703.02136",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 10.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "fisher WER",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "HMM-TDNNtrained with MMI + data augmentation (speed) + iVectors + 3 regularizations + SWBD",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 9.8
              }
            },
            {
              "model_name": "HMM-BLSTMtrained with MMI + data augmentation (speed) + iVectors + 3 regularizations + SWBD",
              "paper_title": null,
              "paper_url": "http://www.danielpovey.com/files/2016_interspeech_mmi.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 9.6
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "chime clean",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "CNN + Bi-RNN + CTC (speech to letters)",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 6.3
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.34
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "chime real",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "CNN + Bi-RNN + CTC (speech to letters)",
              "paper_title": "Deep Speech: Scaling up end-to-end speech recognition",
              "paper_url": "https://arxiv.org/abs/1412.5567",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 67.94
              }
            },
            {
              "model_name": "9-layer model w/ 2 layers of 2D-invariant convolution &amp; 7 recurrent layers, w/ 68M parameters",
              "paper_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
              "paper_url": "https://arxiv.org/abs/1512.02595v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 21.79
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "timit PER",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "(first, modern) HMM-DBN",
              "paper_title": null,
              "paper_url": "http://www.cs.toronto.edu/~asamir/papers/NIPS09.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 23
              }
            },
            {
              "model_name": "Bi-LSTM + skip connections w/ CTC",
              "paper_title": "Speech Recognition with Deep Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1303.5778v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 17.7
              }
            },
            {
              "model_name": "CNN in time and frequency + dropout, 17.6% w/o dropout",
              "paper_title": null,
              "paper_url": "http://www.inf.u-szeged.hu/~tothl/pubs/ICASSP2014.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 16.7
              }
            },
            {
              "model_name": "Bi-RNN + Attention",
              "paper_title": "Attention-Based Models for Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1506.07503",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 17.6
              }
            },
            {
              "model_name": "Hierarchical maxout CNN + Dropout",
              "paper_title": " ",
              "paper_url": "https://link.springer.com/content/pdf/10.1186%2Fs13636-015-0068-3.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 16.5
              }
            },
            {
              "model_name": "RNN-CRF on 24(x3) MFSC",
              "paper_title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition",
              "paper_url": "https://arxiv.org/abs/1603.00223",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 17.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Casual Conversation Turing Test",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "The Loebner Prize scored selection answers",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Rose 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 89.2
              }
            },
            {
              "model_name": "Izar 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 88.3
              }
            },
            {
              "model_name": "Misuku 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 88.3
              }
            },
            {
              "model_name": "Uberbot 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 81.67
              }
            },
            {
              "model_name": "Tutor 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.83
              }
            },
            {
              "model_name": "The Professor 2014",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#contest2014",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.7
              }
            },
            {
              "model_name": "Mitsuku 2015",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results15",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 83.3
              }
            },
            {
              "model_name": "Lisa 2015",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results15",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80
              }
            },
            {
              "model_name": "Izar 2015",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results15",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.7
              }
            },
            {
              "model_name": "Rose 2015",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results15",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75
              }
            },
            {
              "model_name": "Mitsuku 2016",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results16",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90
              }
            },
            {
              "model_name": "Tutor 2016",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results16",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.3
              }
            },
            {
              "model_name": "Rose 2016",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results16",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.5
              }
            },
            {
              "model_name": "Arckon 2016",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results16",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.5
              }
            },
            {
              "model_name": "Katie 2016",
              "paper_title": "AISB - The Society for the Study of Artificial Intelligence and Simulation of Behaviour - Loebner Prize",
              "paper_url": "http://www.aisb.org.uk/events/loebner-prize#Results16",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.7
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Automated Software Engineering",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Card2Code MTG accuracy",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "LPN",
              "paper_title": "Latent Predictor Networks for Code Generation",
              "paper_url": "https://arxiv.org/abs/1603.06744v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 4.8
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Card2Code Hearthstone accuracy",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "LPN",
              "paper_title": "Latent Predictor Networks for Code Generation",
              "paper_url": "https://arxiv.org/abs/1603.06744v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 6.1
              }
            },
            {
              "model_name": "SNM -frontier embed",
              "paper_title": "A Syntactic Neural Model for General-Purpose Code Generation",
              "paper_url": "https://arxiv.org/abs/1704.01696v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 16.7
              }
            },
            {
              "model_name": "Seq2Tree-Unk",
              "paper_title": "A Syntactic Neural Model for General-Purpose Code Generation",
              "paper_url": "https://arxiv.org/abs/1704.01696v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 13.6
              }
            },
            {
              "model_name": "NMT",
              "paper_title": "A Syntactic Neural Model for General-Purpose Code Generation",
              "paper_url": "https://arxiv.org/abs/1704.01696v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 1.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Transfer Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Video Games",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Scientific Result Extraction",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Objective Function Reinforcement Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Explainable Machine Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Image Classification",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Imagenet Image Recognition",
        "description": "",
        "sota": {
          "metrics": [
            "Error rate"
          ],
          "rows": [
            {
              "model_name": "NEC UIUC",
              "paper_title": "ImageNet Large Scale Visual Recognition Competition 2010 (ILSVRC2010)",
              "paper_url": "http://image-net.org/challenges/LSVRC/2010/results",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.28191
              }
            },
            {
              "model_name": "XRCE",
              "paper_title": "ImageNet Large Scale Visual Recognition Competition 2011 (ILSVRC2011)",
              "paper_url": "http://image-net.org/challenges/LSVRC/2011/results",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.2577
              }
            },
            {
              "model_name": "AlexNet / SuperVision",
              "paper_title": "ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012)",
              "paper_url": "http://image-net.org/challenges/LSVRC/2012/results.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.16422
              }
            },
            {
              "model_name": "Clarifai",
              "paper_title": "ImageNet Large Scale Visual Recognition Competition 2013 (ILSVRC2013)",
              "paper_url": "http://www.image-net.org/challenges/LSVRC/2013/results.php",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.11743
              }
            },
            {
              "model_name": "VGG",
              "paper_title": "ImageNet Large Scale Visual Recognition Competition 2014 (ILSVRC2014)",
              "paper_url": "http://image-net.org/challenges/LSVRC/2014/index",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.07405
              }
            },
            {
              "model_name": "withdrawn",
              "paper_title": "Deep Image: Scaling up Image Recognition",
              "paper_url": "https://arxiv.org/abs/1501.02876",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.0458
              }
            },
            {
              "model_name": "MSRA",
              "paper_title": "ILSVRC2015 Results",
              "paper_url": "http://image-net.org/challenges/LSVRC/2015/results",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.03567
              }
            },
            {
              "model_name": "Trimps-Soushen",
              "paper_title": "ILSVRC2016 Results",
              "paper_url": "http://image-net.org/challenges/LSVRC/2016/results",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.02991
              }
            },
            {
              "model_name": "SE-ResNet152 / WMW",
              "paper_title": "ILSVRC2017 Results",
              "paper_url": "http://image-net.org/challenges/LSVRC/2017/results",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Error rate": 0.02251
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "MSRC-21 image semantic labelling (per-class)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "STF",
              "paper_title": "Semantic Texton Forests for Image Categorization and Segmentation",
              "paper_url": "http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.0
              }
            },
            {
              "model_name": "TextonBoost",
              "paper_title": "TextonBoost for Image Understanding",
              "paper_url": "http://research.microsoft.com/pubs/117885/ijcv07a.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 57.0
              }
            },
            {
              "model_name": "HCRF+CO",
              "paper_title": "Graph Cut based Inference with Co-occurrence Statistics",
              "paper_url": "http://research.microsoft.com/en-us/um/people/pkohli/papers/lrkt_eccv2010.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.0
              }
            },
            {
              "model_name": "Auto-Context",
              "paper_title": "Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation",
              "paper_url": "http://pages.ucsd.edu/~ztu/publication/pami_autocontext.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.0
              }
            },
            {
              "model_name": "Are Spatial and Global Constraints Really Necessary for Segmentation?",
              "paper_title": "Are Spatial and Global Constraints Really Necessary for Segmentation?",
              "paper_url": "http://infoscience.epfl.ch/record/169178/files/lucchi_ICCV11.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.0
              }
            },
            {
              "model_name": "FC CRF",
              "paper_title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
              "paper_url": "http://graphics.stanford.edu/projects/densecrf/densecrf.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.0
              }
            },
            {
              "model_name": "Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation",
              "paper_title": "Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation",
              "paper_url": "http://ttic.uchicago.edu/~rurtasun/publications/yao_et_al_cvpr12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 79.0
              }
            },
            {
              "model_name": "Harmony Potentials",
              "paper_title": "Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation",
              "paper_url": "http://link.springer.com/article/10.1007%2Fs11263-011-0449-8",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.0
              }
            },
            {
              "model_name": "Kernelized SSVM/CRF",
              "paper_title": "Structured Image Segmentation using Kernelized Features",
              "paper_url": "https://infoscience.epfl.ch/record/180188/files/top.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 76.0
              }
            },
            {
              "model_name": "PMG",
              "paper_title": "PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer",
              "paper_url": "http://users.cecs.anu.edu.au/~sgould/papers/eccv12-patchGraph.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.8
              }
            },
            {
              "model_name": "MPP",
              "paper_title": "Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation",
              "paper_url": "http://mediatum.ub.tum.de/doc/1175516/1175516.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.2
              }
            },
            {
              "model_name": "Large FC CRF",
              "paper_title": "Large-Scale Semantic Co-Labeling of Image Sets",
              "paper_url": "http://ai2-s2-pdfs.s3.amazonaws.com/daba/eb9185990f65f807c95ff4d09057c2bf1cf0.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.9
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "MSRC-21 image semantic labelling (per-pixel)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "STF",
              "paper_title": "Semantic Texton Forests for Image Categorization and Segmentation",
              "paper_url": "http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.0
              }
            },
            {
              "model_name": "TextonBoost",
              "paper_title": "TextonBoost for Image Understanding",
              "paper_url": "http://research.microsoft.com/pubs/117885/ijcv07a.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.0
              }
            },
            {
              "model_name": "HCRF+CO",
              "paper_title": "Graph Cut based Inference with Co-occurrence Statistics",
              "paper_url": "http://research.microsoft.com/en-us/um/people/pkohli/papers/lrkt_eccv2010.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 87.0
              }
            },
            {
              "model_name": "Auto-Context",
              "paper_title": "Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation",
              "paper_url": "http://pages.ucsd.edu/~ztu/publication/pami_autocontext.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.0
              }
            },
            {
              "model_name": "Are Spatial and Global Constraints Really Necessary for Segmentation?",
              "paper_title": "Are Spatial and Global Constraints Really Necessary for Segmentation?",
              "paper_url": "http://infoscience.epfl.ch/record/169178/files/lucchi_ICCV11.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 85.0
              }
            },
            {
              "model_name": "FC CRF",
              "paper_title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
              "paper_url": "http://graphics.stanford.edu/projects/densecrf/densecrf.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 86.0
              }
            },
            {
              "model_name": "Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation",
              "paper_title": "Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation",
              "paper_url": "http://ttic.uchicago.edu/~rurtasun/publications/yao_et_al_cvpr12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 86.0
              }
            },
            {
              "model_name": "Harmony Potentials",
              "paper_title": "Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation",
              "paper_url": "http://link.springer.com/article/10.1007%2Fs11263-011-0449-8",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 83.0
              }
            },
            {
              "model_name": "Kernelized SSVM/CRF",
              "paper_title": "Structured Image Segmentation using Kernelized Features",
              "paper_url": "https://infoscience.epfl.ch/record/180188/files/top.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.0
              }
            },
            {
              "model_name": "PatchMatchGraph",
              "paper_title": "PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer",
              "paper_url": "http://users.cecs.anu.edu.au/~sgould/papers/eccv12-patchGraph.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 79.0
              }
            },
            {
              "model_name": "MPP",
              "paper_title": "Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation",
              "paper_url": "http://mediatum.ub.tum.de/doc/1175516/1175516.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 85.0
              }
            },
            {
              "model_name": "Large FC CRF",
              "paper_title": "Large-Scale Semantic Co-Labeling of Image Sets",
              "paper_url": "http://ai2-s2-pdfs.s3.amazonaws.com/daba/eb9185990f65f807c95ff4d09057c2bf1cf0.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 86.8
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "CIFAR-100 Image Recognition",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Receptive Field Learning",
              "paper_title": "Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features",
              "paper_url": "http://www.eecs.berkeley.edu/~jiayq/assets/pdf/cvpr12_pooling.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 54.23
              }
            },
            {
              "model_name": "Stochastic Pooling",
              "paper_title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks",
              "paper_url": "https://arxiv.org/abs/1301.3557",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 57.49
              }
            },
            {
              "model_name": "Maxout Networks",
              "paper_title": "Maxout Networks",
              "paper_url": "http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.43
              }
            },
            {
              "model_name": "Tree Priors",
              "paper_title": "Discriminative Transfer Learning with Tree-based Priors",
              "paper_url": "http://www.cs.toronto.edu/~nitish/treebasedpriors.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.15
              }
            },
            {
              "model_name": "Smooth Pooling Regions",
              "paper_title": "Smooth Pooling Regions",
              "paper_url": "http://www.d2.mpi-inf.mpg.de/content/learning-smooth-pooling-regions-visual-recognition",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 56.29
              }
            },
            {
              "model_name": "NiN",
              "paper_title": "Network in Network",
              "paper_url": "http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 64.32
              }
            },
            {
              "model_name": "DNN+Probabilistic Maxout",
              "paper_title": "Improving Deep Neural Networks with Probabilistic Maxout Units",
              "paper_url": "http://openreview.net/document/28d9c3ab-fe88-4836-b898-403d207a037c#28d9c3ab-fe88-4836-b898-403d207a037c",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.86
              }
            },
            {
              "model_name": "Stable and Efficient Representation Learning with Nonnegativity Constraints ",
              "paper_title": "Stable and Efficient Representation Learning with Nonnegativity Constraints ",
              "paper_url": "http://jmlr.org/proceedings/papers/v32/line14.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 60.8
              }
            },
            {
              "model_name": "DSN",
              "paper_title": "Deeply-Supervised Nets",
              "paper_url": "http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 65.43
              }
            },
            {
              "model_name": "SSCNN",
              "paper_title": "Spatially-sparse convolutional neural networks",
              "paper_url": "https://arxiv.org/abs/1409.6070",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.7
              }
            },
            {
              "model_name": "Deep Networks with Internal Selective Attention through Feedback Connections",
              "paper_title": "Deep Networks with Internal Selective Attention through Feedback Connections",
              "paper_url": "http://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.22
              }
            },
            {
              "model_name": "ACN",
              "paper_title": "Striving for Simplicity: The All Convolutional Net",
              "paper_url": "https://arxiv.org/abs/1412.6806",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.29
              }
            },
            {
              "model_name": "NiN+APL",
              "paper_title": "Learning Activation Functions to Improve Deep Neural Networks",
              "paper_url": "https://arxiv.org/abs/1412.6830",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.17
              }
            },
            {
              "model_name": "Fractional MP",
              "paper_title": "Fractional Max-Pooling",
              "paper_url": "https://arxiv.org/abs/1412.6071",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 73.61
              }
            },
            {
              "model_name": "Tuned CNN",
              "paper_title": "Scalable Bayesian Optimization Using Deep Neural Networks",
              "paper_url": "https://arxiv.org/abs/1502.05700",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.6
              }
            },
            {
              "model_name": "RCNN-96",
              "paper_title": "Recurrent Convolutional Neural Network for Object Recognition",
              "paper_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.25
              }
            },
            {
              "model_name": "MLR DNN",
              "paper_title": "Multi-Loss Regularized Deep Neural Network",
              "paper_url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.53
              }
            },
            {
              "model_name": "HD-CNN",
              "paper_title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition",
              "paper_url": "https://sites.google.com/site/homepagezhichengyan/home/hdcnn",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.38
              }
            },
            {
              "model_name": "Deep Representation Learning with Target Coding",
              "paper_title": "Deep Representation Learning with Target Coding",
              "paper_url": "http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 64.77
              }
            },
            {
              "model_name": "DCNN+GFE",
              "paper_title": "Deep Convolutional Neural Networks as Generic Feature Extractors",
              "paper_url": "http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.68
              }
            },
            {
              "model_name": "RReLU",
              "paper_title": "Empirical Evaluation of Rectified Activations in Convolution Network",
              "paper_url": "https://arxiv.org/abs/1505.00853",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 59.75
              }
            },
            {
              "model_name": "MIM",
              "paper_title": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units",
              "paper_url": "https://arxiv.org/abs/1508.00330",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.8
              }
            },
            {
              "model_name": "Tree+Max-Avg pooling",
              "paper_title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",
              "paper_url": "https://arxiv.org/abs/1509.08985",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.63
              }
            },
            {
              "model_name": "SWWAE",
              "paper_title": "Stacked What-Where Auto-encoders",
              "paper_url": "https://arxiv.org/abs/1506.02351",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.12
              }
            },
            {
              "model_name": "BNM NiN",
              "paper_title": "Batch-normalized Maxout Network in Network",
              "paper_url": "https://arxiv.org/abs/1511.02583",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.14
              }
            },
            {
              "model_name": "CMsC",
              "paper_title": "Competitive Multi-scale Convolution",
              "paper_url": "https://arxiv.org/abs/1511.05635",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.44
              }
            },
            {
              "model_name": "Spectral Representations for Convolutional Neural Networks",
              "paper_title": "Spectral Representations for Convolutional Neural Networks",
              "paper_url": "http://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.4
              }
            },
            {
              "model_name": "VDN",
              "paper_title": "Training Very Deep Networks",
              "paper_url": "http://people.idsia.ch/~rupesh/very_deep_learning/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.76
              }
            },
            {
              "model_name": "Fitnet4-LSUV",
              "paper_title": "All you need is a good init",
              "paper_url": "https://arxiv.org/abs/1511.06422",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.34
              }
            },
            {
              "model_name": "Exponential Linear Units",
              "paper_title": "Fast and Accurate Deep Network Learning by Exponential Linear Units",
              "paper_url": "https://arxiv.org/abs/1511.07289",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.72
              }
            },
            {
              "model_name": "Universum Prescription",
              "paper_title": "Universum Prescription: Regularization using Unlabeled Data",
              "paper_url": "https://arxiv.org/abs/1511.03719",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.16
              }
            },
            {
              "model_name": "ResNet-1001",
              "paper_title": "Identity Mappings in Deep Residual Networks",
              "paper_url": "https://arxiv.org/abs/1603.05027",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.28999999999999
              }
            },
            {
              "model_name": "ResNet+ELU",
              "paper_title": "Deep Residual Networks with Exponential Linear Unit",
              "paper_url": "https://arxiv.org/abs/1604.04112",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 73.45
              }
            },
            {
              "model_name": "Evolution",
              "paper_title": "Large-Scale Evolution of Image Classifiers",
              "paper_url": "https://arxiv.org/abs/1703.01041",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.0
              }
            },
            {
              "model_name": "Deep Complex",
              "paper_title": "Deep Complex Networks",
              "paper_url": "https://arxiv.org/abs/1705.09792v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.91
              }
            },
            {
              "model_name": "NiN+Superclass+CDJ",
              "paper_title": "Deep Convolutional Decision Jungle for Image Classification",
              "paper_url": "https://arxiv.org/abs/1706.02003",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "CIFAR-10 Image Recognition",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Hierarchical Kernel Descriptors",
              "paper_title": "Object Recognition with Hierarchical Kernel Descriptors",
              "paper_url": "http://research.cs.washington.edu/istc/lfb/paper/cvpr11.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.0
              }
            },
            {
              "model_name": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning ",
              "paper_title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning ",
              "paper_url": "http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 79.6
              }
            },
            {
              "model_name": "MCDNN",
              "paper_title": "Multi-Column Deep Neural Networks for Image Classification ",
              "paper_url": "http://www.idsia.ch/~ciresan/data/cvpr2012.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 88.79
              }
            },
            {
              "model_name": "Local Transformations",
              "paper_title": "Learning Invariant Representations with Local Transformations",
              "paper_url": "http://icml.cc/2012/papers/659.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.2
              }
            },
            {
              "model_name": "Improving neural networks by preventing co-adaptation of feature detectors",
              "paper_title": "Improving neural networks by preventing co-adaptation of feature detectors",
              "paper_url": "https://arxiv.org/abs/1207.0580",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 84.4
              }
            },
            {
              "model_name": "GP EI",
              "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms ",
              "paper_url": "http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.5
              }
            },
            {
              "model_name": "DCNN",
              "paper_title": "ImageNet Classification with Deep Convolutional Neural Networks",
              "paper_url": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 89.0
              }
            },
            {
              "model_name": "Discriminative Learning of Sum-Product Networks",
              "paper_title": "Discriminative Learning of Sum-Product Networks",
              "paper_url": "http://papers.nips.cc/paper/4516-discriminative-learning-of-sum-product-networks",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 83.96
              }
            },
            {
              "model_name": "Learning with Recursive Perceptual Representations",
              "paper_title": "Learning with Recursive Perceptual Representations",
              "paper_url": "http://papers.nips.cc/paper/4747-learning-with-recursive-perceptual-representations",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 79.7
              }
            },
            {
              "model_name": "Stochastic Pooling",
              "paper_title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks",
              "paper_url": "https://arxiv.org/abs/1301.3557",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 84.87
              }
            },
            {
              "model_name": "DropConnect",
              "paper_title": "Regularization of Neural Networks using DropConnect",
              "paper_url": "http://cs.nyu.edu/~wanli/dropc/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.68
              }
            },
            {
              "model_name": "Maxout Networks",
              "paper_title": "Maxout Networks",
              "paper_url": "http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.65
              }
            },
            {
              "model_name": "Smooth Pooling Regions",
              "paper_title": "Learning Smooth Pooling Regions for Visual Recognition",
              "paper_url": "http://www.d2.mpi-inf.mpg.de/content/learning-smooth-pooling-regions-visual-recognition",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 80.02
              }
            },
            {
              "model_name": "NiN",
              "paper_title": "Network In Network",
              "paper_url": "http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.2
              }
            },
            {
              "model_name": "DNN+Probabilistic Maxout",
              "paper_title": "Improving Deep Neural Networks with Probabilistic Maxout Units",
              "paper_url": "http://openreview.net/document/28d9c3ab-fe88-4836-b898-403d207a037c#28d9c3ab-fe88-4836-b898-403d207a037c",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.61
              }
            },
            {
              "model_name": "Nonnegativity Constraints ",
              "paper_title": "Stable and Efficient Representation Learning with Nonnegativity Constraints ",
              "paper_url": "http://jmlr.org/proceedings/papers/v32/line14.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.9
              }
            },
            {
              "model_name": "PCANet",
              "paper_title": "PCANet: A Simple Deep Learning Baseline for Image Classification?",
              "paper_url": "https://arxiv.org/abs/1404.3606",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 78.67
              }
            },
            {
              "model_name": "DSN",
              "paper_title": "Deeply-Supervised Nets",
              "paper_url": "http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.78
              }
            },
            {
              "model_name": "CKN",
              "paper_title": "Convolutional Kernel Networks",
              "paper_url": "https://arxiv.org/abs/1406.3332",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.18
              }
            },
            {
              "model_name": "SSCNN",
              "paper_title": "Spatially-sparse convolutional neural networks",
              "paper_url": "https://arxiv.org/abs/1409.6070",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.72
              }
            },
            {
              "model_name": "Deep Networks with Internal Selective Attention through Feedback Connections",
              "paper_title": "Deep Networks with Internal Selective Attention through Feedback Connections",
              "paper_url": "http://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 90.78
              }
            },
            {
              "model_name": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
              "paper_title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
              "paper_url": "http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 82.0
              }
            },
            {
              "model_name": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
              "paper_title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
              "paper_url": "https://arxiv.org/abs/1412.6597",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 86.7
              }
            },
            {
              "model_name": "ACN",
              "paper_title": "Striving for Simplicity: The All Convolutional Net",
              "paper_url": "https://arxiv.org/abs/1412.6806",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 95.59
              }
            },
            {
              "model_name": "NiN+APL",
              "paper_title": "Learning Activation Functions to Improve Deep Neural Networks",
              "paper_url": "https://arxiv.org/abs/1412.6830",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 92.49
              }
            },
            {
              "model_name": "Fractional MP",
              "paper_title": "Fractional Max-Pooling",
              "paper_url": "https://arxiv.org/abs/1412.6071",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 96.53
              }
            },
            {
              "model_name": "Tuned CNN",
              "paper_title": "Scalable Bayesian Optimization Using Deep Neural Networks",
              "paper_url": "https://arxiv.org/abs/1502.05700",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.63
              }
            },
            {
              "model_name": "APAC",
              "paper_title": "APAC: Augmented PAttern Classification with Neural Networks",
              "paper_url": "https://arxiv.org/abs/1505.03229",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 89.67
              }
            },
            {
              "model_name": "FLSCNN",
              "paper_title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network",
              "paper_url": "https://arxiv.org/abs/1503.04596",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 75.86
              }
            },
            {
              "model_name": "RCNN-96",
              "paper_title": "Recurrent Convolutional Neural Network for Object Recognition",
              "paper_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 92.91
              }
            },
            {
              "model_name": "ReNet",
              "paper_title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks",
              "paper_url": "https://arxiv.org/abs/1505.00393",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 87.65
              }
            },
            {
              "model_name": "cifar.torch",
              "paper_title": "cifar.torch",
              "paper_url": "http://torch.ch/blog/2015/07/30/cifar.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 92.45
              }
            },
            {
              "model_name": "MLR DNN",
              "paper_title": "Multi-Loss Regularized Deep Neural Network",
              "paper_url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.88
              }
            },
            {
              "model_name": "ELC",
              "paper_title": "Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves",
              "paper_url": "http://aad.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.19
              }
            },
            {
              "model_name": "DCNN+GFE",
              "paper_title": "Deep Convolutional Neural Networks as Generic Feature Extractors",
              "paper_url": "http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 89.14
              }
            },
            {
              "model_name": "RReLU",
              "paper_title": "Empirical Evaluation of Rectified Activations in Convolution Network",
              "paper_url": "https://arxiv.org/abs/1505.00853",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 88.8
              }
            },
            {
              "model_name": "MIM",
              "paper_title": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units",
              "paper_url": "https://arxiv.org/abs/1508.00330",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.48
              }
            },
            {
              "model_name": "Tree+Max-Avg pooling",
              "paper_title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",
              "paper_url": "https://arxiv.org/abs/1509.08985",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.95
              }
            },
            {
              "model_name": "SWWAE",
              "paper_title": "Stacked What-Where Auto-encoders",
              "paper_url": "https://arxiv.org/abs/1506.02351",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 92.23
              }
            },
            {
              "model_name": "BNM NiN",
              "paper_title": "Batch-normalized Maxout Network in Network",
              "paper_url": "https://arxiv.org/abs/1511.02583",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.25
              }
            },
            {
              "model_name": "CMsC",
              "paper_title": "Competitive Multi-scale Convolution",
              "paper_url": "https://arxiv.org/abs/1511.05635",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.13
              }
            },
            {
              "model_name": "VDN",
              "paper_title": "Training Very Deep Networks",
              "paper_url": "http://people.idsia.ch/~rupesh/very_deep_learning/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 92.4
              }
            },
            {
              "model_name": "BinaryConnect",
              "paper_title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
              "paper_url": "http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.73
              }
            },
            {
              "model_name": "Spectral Representations for Convolutional Neural Networks",
              "paper_title": "Spectral Representations for Convolutional Neural Networks",
              "paper_url": "http://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 91.4
              }
            },
            {
              "model_name": "DRL",
              "paper_title": "Deep Residual Learning for Image Recognition",
              "paper_url": "https://arxiv.org/abs/1512.03385",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.57
              }
            },
            {
              "model_name": "Fitnet4-LSUV",
              "paper_title": "All you need is a good init",
              "paper_url": "https://arxiv.org/abs/1511.06422",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 94.16
              }
            },
            {
              "model_name": "Exponential Linear Units",
              "paper_title": "Fast and Accurate Deep Network Learning by Exponential Linear Units",
              "paper_url": "https://arxiv.org/abs/1511.07289",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.45
              }
            },
            {
              "model_name": "Universum Prescription",
              "paper_title": "Universum Prescription: Regularization using Unlabeled Data",
              "paper_url": "https://arxiv.org/abs/1511.03719",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 93.34
              }
            },
            {
              "model_name": "ResNet-1001",
              "paper_title": "Identity Mappings in Deep Residual Networks",
              "paper_url": "https://arxiv.org/abs/1603.05027",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 95.38
              }
            },
            {
              "model_name": "ResNet+ELU",
              "paper_title": "Deep Residual Networks with Exponential Linear Unit",
              "paper_url": "https://arxiv.org/abs/1604.04112",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 94.38
              }
            },
            {
              "model_name": "Neural Architecture Search",
              "paper_title": "Neural Architecture Search with Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.01578v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 96.35
              }
            },
            {
              "model_name": "Evolution ensemble",
              "paper_title": "Large-Scale Evolution of Image Classifiers",
              "paper_url": "https://arxiv.org/abs/1703.01041",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 95.6
              }
            },
            {
              "model_name": "Evolution",
              "paper_title": "Large-Scale Evolution of Image Classifiers",
              "paper_url": "https://arxiv.org/abs/1703.01041",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 94.6
              }
            },
            {
              "model_name": "Deep Complex",
              "paper_title": "Deep Complex Networks",
              "paper_url": "https://arxiv.org/abs/1705.09792v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 94.4
              }
            },
            {
              "model_name": "RL+NT",
              "paper_title": "Reinforcement Learning for Architecture Search by Network Transformation",
              "paper_url": "https://arxiv.org/abs/1707.04873",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 94.6
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Street View House Numbers (SVHN)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "Convolutional neural networks applied to house numbers digit classification",
              "paper_title": "Convolutional neural networks applied to house numbers digit classification",
              "paper_url": "http://yann.lecun.com/exdb/publis/pdf/sermanet-icpr-12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 4.9
              }
            },
            {
              "model_name": "Stochastic Pooling",
              "paper_title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks",
              "paper_url": "https://arxiv.org/abs/1301.3557",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.8
              }
            },
            {
              "model_name": "Maxout",
              "paper_title": "Maxout Networks",
              "paper_url": "http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.47
              }
            },
            {
              "model_name": "Regularization of Neural Networks using DropConnect",
              "paper_title": "Regularization of Neural Networks using DropConnect",
              "paper_url": "http://cs.nyu.edu/~wanli/dropc/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.94
              }
            },
            {
              "model_name": "NiN",
              "paper_title": "Network in Network",
              "paper_url": "http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.35
              }
            },
            {
              "model_name": "DCNN",
              "paper_title": "Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks",
              "paper_url": "http://openreview.net/document/0c571b22-f4b6-4d58-87e4-99d7de42a893#0c571b22-f4b6-4d58-87e4-99d7de42a893",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.16
              }
            },
            {
              "model_name": "DSN",
              "paper_title": "Deeply-Supervised Nets",
              "paper_url": "http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.92
              }
            },
            {
              "model_name": "FLSCNN",
              "paper_title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network",
              "paper_url": "https://arxiv.org/abs/1503.04596",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.96
              }
            },
            {
              "model_name": "RCNN-96",
              "paper_title": "Recurrent Convolutional Neural Network for Object Recognition",
              "paper_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.77
              }
            },
            {
              "model_name": "ReNet",
              "paper_title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks",
              "paper_url": "https://arxiv.org/abs/1505.00393",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.38
              }
            },
            {
              "model_name": "MLR DNN",
              "paper_title": "Multi-Loss Regularized Deep Neural Network",
              "paper_url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.92
              }
            },
            {
              "model_name": "MIM",
              "paper_title": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units",
              "paper_url": "https://arxiv.org/abs/1508.00330",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.97
              }
            },
            {
              "model_name": "Tree+Max-Avg pooling",
              "paper_title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",
              "paper_url": "https://arxiv.org/abs/1509.08985",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.69
              }
            },
            {
              "model_name": "BNM NiN",
              "paper_title": "Batch-normalized Maxout Network in Network",
              "paper_url": "https://arxiv.org/abs/1511.02583",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.81
              }
            },
            {
              "model_name": "CMsC",
              "paper_title": "Competitive Multi-scale Convolution",
              "paper_url": "https://arxiv.org/abs/1511.05635",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.76
              }
            },
            {
              "model_name": "BinaryConnect",
              "paper_title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
              "paper_url": "http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 2.15
              }
            },
            {
              "model_name": "Deep Complex",
              "paper_title": "Deep Complex Networks",
              "paper_url": "https://arxiv.org/abs/1705.09792v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 3.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "MNIST handwritten digit recognition",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage error"
          ],
          "rows": [
            {
              "model_name": "Shape contexts",
              "paper_title": "Shape matching and object recognition using shape contexts",
              "paper_url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=B2AAC2BC3824F19757CAC66986D5F3FF?doi=10.1.1.18.8852&rep=rep1&type=pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.63
              }
            },
            {
              "model_name": "ISVM",
              "paper_title": "Training Invariant Support Vector Machines",
              "paper_url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.9924&rep=rep1&type=pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.56
              }
            },
            {
              "model_name": "CNN",
              "paper_title": "Convolutional Neural Networks",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.19
              }
            },
            {
              "model_name": "CNN+Gabor Filters",
              "paper_title": "Handwritten Digit Recognition using Convolutional Neural Networks and Gabor Filters",
              "paper_url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.6559&rep=rep1&type=pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.68
              }
            },
            {
              "model_name": "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis",
              "paper_title": "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis",
              "paper_url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=D1C7D701BD39935473808DA5A93426C5?doi=10.1.1.160.8494&rep=rep1&type=pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.4
              }
            },
            {
              "model_name": "Reducing the dimensionality of data with neural networks",
              "paper_title": "Reducing the dimensionality of data with neural networks",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.2
              }
            },
            {
              "model_name": "Energy-Based Sparse Represenation",
              "paper_title": "Efficient Learning of Sparse Representations with an Energy-Based Model",
              "paper_url": "http://papers.nips.cc/paper/3112-efficient-learning-of-sparse-representations-with-an-energy-based-model",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.39
              }
            },
            {
              "model_name": "invariant feature hierarchies",
              "paper_title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
              "paper_url": "http://yann.lecun.com/exdb/publis/pdf/ranzato-cvpr-07.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.62
              }
            },
            {
              "model_name": "Deformation Models",
              "paper_title": "Deformation Models for Image Recognition",
              "paper_url": "http://www.keysers.net/daniel/files/Keysers--Deformation-Models--TPAMI2007.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.54
              }
            },
            {
              "model_name": "Trainable feature extractor",
              "paper_title": "A trainable feature extractor for handwritten digit recognition",
              "paper_url": "http://hal.inria.fr/docs/00/05/75/61/PDF/LauerSuenBlochPR.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.54
              }
            },
            {
              "model_name": "Deep learning via semi-supervised embedding",
              "paper_title": "Deep learning via semi-supervised embedding",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.5
              }
            },
            {
              "model_name": "DBN",
              "paper_title": "CS81: Learning words with Deep Belief Networks",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.12
              }
            },
            {
              "model_name": "Sparse Coding",
              "paper_title": "Simple Methods for High-Performance Digit Recognition Based on Sparse Coding",
              "paper_url": "http://www.inb.uni-luebeck.de/publikationen/pdfs/LaBaMa08c.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.59
              }
            },
            {
              "model_name": "Deep Boltzmann Machines",
              "paper_title": "Deep Boltzmann Machines",
              "paper_url": "http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.95
              }
            },
            {
              "model_name": "Large-Margin kNN",
              "paper_title": "Large-Margin kNN Classification using a Deep Encoder Network",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.94
              }
            },
            {
              "model_name": "CDBN",
              "paper_title": "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations",
              "paper_url": "",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.82
              }
            },
            {
              "model_name": "The Best Multi-Stage Architecture",
              "paper_title": "What is the Best Multi-Stage Architecture for Object Recognition?",
              "paper_url": "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.53
              }
            },
            {
              "model_name": "DBSNN",
              "paper_title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition",
              "paper_url": "https://arxiv.org/abs/1003.0358",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.35
              }
            },
            {
              "model_name": "Supervised Translation-Invariant Sparse Coding",
              "paper_title": "Supervised Translation-Invariant Sparse Coding",
              "paper_url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.206.339&rep=rep1&type=pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.84
              }
            },
            {
              "model_name": "On Optimization Methods for Deep Learning",
              "paper_title": "On Optimization Methods for Deep Learning",
              "paper_url": "http://ai.stanford.edu/~quocle/LeNgiCoaLahProNg11.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.69
              }
            },
            {
              "model_name": "Receptive Field Learning",
              "paper_title": "Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features",
              "paper_url": "http://www.icsi.berkeley.edu/pubs/vision/beyondspatial12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.64
              }
            },
            {
              "model_name": "MCDNN",
              "paper_title": "Multi-column Deep Neural Networks for Image Classification ",
              "paper_url": "http://www.idsia.ch/~ciresan/data/cvpr2012.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.23
              }
            },
            {
              "model_name": "COSFIRE",
              "paper_title": "Trainable COSFIRE Filters for Keypoint Detection and Pattern Recognition",
              "paper_url": "http://www.cs.rug.nl/~george/articles/PAMI2013.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.52
              }
            },
            {
              "model_name": "Maxout Networks",
              "paper_title": "Maxout Networks",
              "paper_url": "http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.45
              }
            },
            {
              "model_name": "DropConnect",
              "paper_title": "Regularization of Neural Networks using DropConnect",
              "paper_url": "http://cs.nyu.edu/~wanli/dropc/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.21
              }
            },
            {
              "model_name": "Sparse Activity and Sparse Connectivity in Supervised Learning",
              "paper_title": "Sparse Activity and Sparse Connectivity in Supervised Learning",
              "paper_url": "http://jmlr.org/papers/v14/thom13a.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.75
              }
            },
            {
              "model_name": "NiN",
              "paper_title": "Network in Network",
              "paper_url": "http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.47
              }
            },
            {
              "model_name": "PCANet",
              "paper_title": "PCANet: A Simple Deep Learning Baseline for Image Classification?",
              "paper_url": "https://arxiv.org/abs/1404.3606",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.62
              }
            },
            {
              "model_name": "StrongNet",
              "paper_title": "StrongNet: mostly unsupervised image recognition with strong neurons",
              "paper_url": "http://www.alglib.net/articles/tr-20140813-strongnet.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.1
              }
            },
            {
              "model_name": "DSN",
              "paper_title": "Deeply-Supervised Nets",
              "paper_url": "http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.39
              }
            },
            {
              "model_name": "CKN",
              "paper_title": "Convolutional Kernel Networks",
              "paper_url": "https://arxiv.org/abs/1406.3332",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.39
              }
            },
            {
              "model_name": "Explaining and Harnessing Adversarial Examples",
              "paper_title": "Explaining and Harnessing Adversarial Examples",
              "paper_url": "https://arxiv.org/abs/1412.6572",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.78
              }
            },
            {
              "model_name": "Fractional MP",
              "paper_title": "Fractional Max-Pooling",
              "paper_url": "https://arxiv.org/abs/1412.6071",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.32
              }
            },
            {
              "model_name": "C-SVDDNet",
              "paper_title": "C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning",
              "paper_url": "https://arxiv.org/abs/1412.7259",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.35
              }
            },
            {
              "model_name": "HOPE",
              "paper_title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks",
              "paper_url": "https://arxiv.org/abs/1502.00702",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.4
              }
            },
            {
              "model_name": "APAC",
              "paper_title": "APAC: Augmented PAttern Classification with Neural Networks",
              "paper_url": "https://arxiv.org/abs/1505.03229",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.23
              }
            },
            {
              "model_name": "FLSCNN",
              "paper_title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network",
              "paper_url": "https://arxiv.org/abs/1503.04596",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.37
              }
            },
            {
              "model_name": "RCNN-96",
              "paper_title": "Recurrent Convolutional Neural Network for Object Recognition",
              "paper_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.31
              }
            },
            {
              "model_name": "ReNet",
              "paper_title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks",
              "paper_url": "https://arxiv.org/abs/1505.00393",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.45
              }
            },
            {
              "model_name": "Deep Fried Convnets",
              "paper_title": "Deep Fried Convnets",
              "paper_url": "http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.71
              }
            },
            {
              "model_name": "MLR DNN",
              "paper_title": "Multi-Loss Regularized Deep Neural Network",
              "paper_url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.42
              }
            },
            {
              "model_name": "DCNN+GFE",
              "paper_title": "Deep Convolutional Neural Networks as Generic Feature Extractors",
              "paper_url": "http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.46
              }
            },
            {
              "model_name": "MIM",
              "paper_title": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units",
              "paper_url": "https://arxiv.org/abs/1508.00330",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.35
              }
            },
            {
              "model_name": "Tree+Max-Avg pooling",
              "paper_title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",
              "paper_url": "https://arxiv.org/abs/1509.08985",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.29
              }
            },
            {
              "model_name": "BNM NiN",
              "paper_title": "Batch-normalized Maxout Network in Network",
              "paper_url": "https://arxiv.org/abs/1511.02583",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.24
              }
            },
            {
              "model_name": "CMsC",
              "paper_title": "Competitive Multi-scale Convolution",
              "paper_url": "https://arxiv.org/abs/1511.05635",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.33
              }
            },
            {
              "model_name": "BinaryConnect",
              "paper_title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
              "paper_url": "http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.01
              }
            },
            {
              "model_name": "VDN",
              "paper_title": "Training Very Deep Networks",
              "paper_url": "http://people.idsia.ch/~rupesh/very_deep_learning/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.45
              }
            },
            {
              "model_name": "Convolutional Clustering",
              "paper_title": "Convolutional Clustering for Unsupervised Learning",
              "paper_url": "https://arxiv.org/abs/1511.06241",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 1.4
              }
            },
            {
              "model_name": "Fitnet-LSUV-SVM",
              "paper_title": "All you need is a good init",
              "paper_url": "https://arxiv.org/abs/1511.06422",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage error": 0.38
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "STL-10 Image Recognition",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Receptive Fields",
              "paper_title": "Selecting Receptive Fields in Deep Networks ",
              "paper_url": "http://www.stanford.edu/~acoates/papers/coatesng_nips_2011.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 60.1
              }
            },
            {
              "model_name": "Invariant Representations with Local Transformations",
              "paper_title": "Learning Invariant Representations with Local Transformations",
              "paper_url": "http://web.eecs.umich.edu/~honglak/icml12-invariantFeatureLearning.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 58.7
              }
            },
            {
              "model_name": "RGB-D Based Object Recognition",
              "paper_title": "Unsupervised Feature Learning for RGB-D Based Object Recognition",
              "paper_url": "http://homes.cs.washington.edu/~lfb/paper/iser12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 64.5
              }
            },
            {
              "model_name": "Simulated Fixations",
              "paper_title": "Deep Learning of Invariant Features via Simulated Fixations in Video",
              "paper_url": "http://papers.nips.cc/paper/4730-deep-learning-of-invariant-features-via-simulated-fixations-in-video",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.0
              }
            },
            {
              "model_name": "Discriminative Learning of Sum-Product Networks",
              "paper_title": "Discriminative Learning of Sum-Product Networks",
              "paper_url": "http://homes.cs.washington.edu/~rcg/papers/dspn.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 62.3
              }
            },
            {
              "model_name": "Deep Learning of Invariant Features via Simulated Fixations in Video",
              "paper_title": "Deep Learning of Invariant Features via Simulated Fixations in Video",
              "paper_url": "http://ai.stanford.edu/~wzou/nips_ZouZhuNgYu12.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 56.5
              }
            },
            {
              "model_name": "Pooling-Invariant",
              "paper_title": "Pooling-Invariant Image Feature Learning ",
              "paper_url": "https://arxiv.org/abs/1302.5056v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 58.28
              }
            },
            {
              "model_name": "Multi-Task Bayesian Optimization",
              "paper_title": "Multi-Task Bayesian Optimization",
              "paper_url": "http://hips.seas.harvard.edu/files/swersky-multi-nips-2013.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.1
              }
            },
            {
              "model_name": "No more meta-parameter tuning in unsupervised sparse feature learning",
              "paper_title": "No more meta-parameter tuning in unsupervised sparse feature learning",
              "paper_url": "https://arxiv.org/abs/1402.5766",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.0
              }
            },
            {
              "model_name": "Nonnegativity Constraints ",
              "paper_title": "Stable and Efficient Representation Learning with Nonnegativity Constraints ",
              "paper_url": "http://jmlr.org/proceedings/papers/v32/line14.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.9
              }
            },
            {
              "model_name": "DFF Committees",
              "paper_title": "Committees of deep feedforward networks trained with few data",
              "paper_url": "https://arxiv.org/abs/1406.5947",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.0
              }
            },
            {
              "model_name": "CKN",
              "paper_title": "Convolutional Kernel Networks",
              "paper_url": "https://arxiv.org/abs/1406.3332",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 62.32
              }
            },
            {
              "model_name": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
              "paper_title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
              "paper_url": "http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.8
              }
            },
            {
              "model_name": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
              "paper_title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
              "paper_url": "https://arxiv.org/abs/1412.6597",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.2
              }
            },
            {
              "model_name": "C-SVDDNet",
              "paper_title": "C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning",
              "paper_url": "https://arxiv.org/abs/1412.7259",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.23
              }
            },
            {
              "model_name": "Deep Representation Learning with Target Coding",
              "paper_title": "Deep Representation Learning with Target Coding",
              "paper_url": "http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 73.15
              }
            },
            {
              "model_name": "SWWAE",
              "paper_title": "Stacked What-Where Auto-encoders",
              "paper_url": "https://arxiv.org/abs/1506.02351",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.33
              }
            },
            {
              "model_name": "Convolutional Clustering",
              "paper_title": "Convolutional Clustering for Unsupervised Learning",
              "paper_url": "https://arxiv.org/abs/1511.06241",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.1
              }
            },
            {
              "model_name": "CC-GAN\u00b2",
              "paper_title": "Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks",
              "paper_url": "https://arxiv.org/abs/1611.06430v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 77.79
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Scientific Paper Comprehension",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Scientific Question Answering",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "NY Regents 4th Grade Science Exams",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Aristo (ALL)",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.3
              }
            },
            {
              "model_name": "PMI",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 60.7
              }
            },
            {
              "model_name": "IR",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 60.6
              }
            },
            {
              "model_name": "SVM",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 55.4
              }
            },
            {
              "model_name": "RULE",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 54.3
              }
            },
            {
              "model_name": "Praline",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 47.5
              }
            },
            {
              "model_name": "ILP",
              "paper_title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
              "paper_url": "https://pdfs.semanticscholar.org/478b/4a5123bd5fda98bb35e6317d7f3555fec97d.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 43.8
              }
            },
            {
              "model_name": "TableILP+IR+PMI",
              "paper_title": "Question Answering via Integer Programming over Semi-Structured Knowledge",
              "paper_url": "https://arxiv.org/abs/1604.06076",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.0
              }
            },
            {
              "model_name": "TableILP",
              "paper_title": "Question Answering via Integer Programming over Semi-Structured Knowledge",
              "paper_url": "https://arxiv.org/abs/1604.06076",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Automated Interrogation",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Vision",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Strategy Game Rule Learning",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Predictable Artificial General Intelligence",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Bias Avoidance",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Functional Robustness",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Security Bug Detection",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Simple Video Games",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Atari 2600 Alien",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 103.2
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 939.2
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3069.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 813.5
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1620.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 634.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4461.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3747.7
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1486.5
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1033.4
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 823.7
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4203.8
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1334.7
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3213.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3941.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 945.3
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 518.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 182.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 994.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3166.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Amidar",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 183.6
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 103.4
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 739.5
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 189.2
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 978.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 178.4
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2354.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1793.3
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 172.7
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 238.4
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 169.1
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1838.9
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 129.1
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 782.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2296.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 283.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 263.9
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 173.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 112.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1735.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Assault",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 537.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 628.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3359.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1195.8
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4280.4
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3489.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5393.2
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4621.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3994.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10950.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6060.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7672.1
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6548.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9011.6
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11477.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14497.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5474.9
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3746.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1673.9
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7203.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Asterix",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1332.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 987.3
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6012.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3324.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4359.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3170.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28188.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17356.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15840.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 364200.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16837.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31527.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22484.5
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18919.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 375080.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22140.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17244.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6723.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1440.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 406211.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Asteroids",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 89.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 907.3
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1629.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 933.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1458.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1364.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2837.7
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2035.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 734.7
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1193.2
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1021.9
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2654.3
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1745.1
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2869.3
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1192.7
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5093.1
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4474.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3009.4
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1562.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1516.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Atlantis",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 852.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 62687.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 85641.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 629166.5
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 292491.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 279987.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 445360.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 382572.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 106056.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 423252.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 319688.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 357324.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 330647.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 340076.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 395762.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 911091.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 875822.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 772392.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1267410.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 841075.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Bank Heist",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 67.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 190.8
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 429.7
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 399.4
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 455.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 312.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1611.9
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1129.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1030.6
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1004.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 886.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1054.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 876.6
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1103.3
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1503.1
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 970.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 946.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 932.8
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 225.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 976.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Battle Zone",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16.2
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15820.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26300.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19938.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 29900.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23750.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 37150.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31700.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31320.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30650.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24740.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31530.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 25520.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8220.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 35520.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20760.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12950.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11340.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16600.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28742.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Beam Rider",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1743.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 929.4
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5184
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6846.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3822.1
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9743.2
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8627.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14591.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13772.8
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12164.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 37412.2
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17417.2
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31181.3
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23384.2
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8299.4
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30276.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24622.2
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22707.9
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13235.9
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 744.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14074.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Berzerk",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 585.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 493.4
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1472.6
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1225.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 910.6
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2178.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1011.1
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1305.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 865.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1199.6
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3409.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1433.4
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 862.2
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 817.9
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 686.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1645.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Bowling",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 36.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 43.9
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 42.4
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 54.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56.5
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 50.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 68.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 65.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 65.5
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 69.6
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 50.4
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 52.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 47.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 102.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 46.7
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 41.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 36.2
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 35.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 81.8
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Boxing",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9.8
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 44.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 71.8
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 74.2
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 88.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70.3
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 99.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 91.6
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 77.3
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 79.2
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73.5
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 95.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 72.3
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 99.3
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 98.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 59.8
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 37.3
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.7
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 49.8
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 97.8
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Breakout",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6.1
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5.2
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 225
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 401.2
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 313.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 385.5
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 354.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 418.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 411.6
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 345.3
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 368.9
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 354.6
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 373.9
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 343.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 344.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 366.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 766.8
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 681.9
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 551.6
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 748.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Centipede",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4647.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8803.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8309.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6296.9
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4657.7
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3973.9
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7561.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5409.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4881.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5570.2
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3853.5
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4463.2
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3489.1
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 49065.8
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7687.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3755.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3306.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1997.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7783.9
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9646.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Chopper Command",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1582.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6687.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3191.8
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6126.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5017.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11215.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5809.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3784.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8058.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3495.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8600.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4635.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 775.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13185.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10150.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7021.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4669.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3710.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15600.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Crazy Climber",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 149.8
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23411.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 114103.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 65451.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 110763.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 98128.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 143570.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 124566.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 117282.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 127853.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 113782.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 141161.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 127512.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 119679.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 162224.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 138518.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 112646.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 101624.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26430.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 179877.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Demon Attack",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 520.5
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9711.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14880.1
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12550.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12149.4
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 60813.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 58044.2
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56322.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73371.3
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 69803.4
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 71846.4
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 61277.5
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 63644.9
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 72878.6
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 115201.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 113308.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 84997.5
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1166.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 130955.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Double Dunk",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -16.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13.1
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -18.1
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -11.3
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -6.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -6.6
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.8
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -5.5
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.3
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -10.7
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -11.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -12.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.2
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Enduro",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 159.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 129.1
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 661
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 301.8
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 71.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 729.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 626.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2258.2
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2077.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1211.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2223.9
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1216.6
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2093.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1831.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2002.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2306.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -82.2
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -82.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -82.5
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 95.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3454.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Fishing Derby",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -85.1
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -89.5
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.8
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.6
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.9
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 46.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.1
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 39.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9.8
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 45.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 41.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22.6
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13.6
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -49.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8.9
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Freeway",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19.7
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19.1
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30.3
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10.2
              }
            },
            {
              "model_name": "MP-EB",
              "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
              "paper_url": "https://arxiv.org/abs/1507.00814",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 27.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30.8
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26.9
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.3
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.2
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.4
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.1
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30.48
              }
            },
            {
              "model_name": "TRPO-hash",
              "paper_title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.04717",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34.0
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.0
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31.7
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31.0
              }
            },
            {
              "model_name": "Sarsa-\u03b5",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 29.9
              }
            },
            {
              "model_name": "Sarsa-\u03c6-EB",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33.9
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Frostbite",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 180.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 216.9
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 328.3
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 426.6
              }
            },
            {
              "model_name": "MP-EB",
              "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
              "paper_url": "https://arxiv.org/abs/1507.00814",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 507.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 797.4
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 496.1
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4672.8
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2332.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1683.3
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4038.4
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1448.1
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4380.1
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3510.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3469.6
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7413.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 197.6
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 190.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 180.1
              }
            },
            {
              "model_name": "TRPO-hash",
              "paper_title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.04717",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5214.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 370.0
              }
            },
            {
              "model_name": "Sarsa-\u03c6-EB",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2770.1
              }
            },
            {
              "model_name": "Sarsa-\u03b5",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1394.3
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3965.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Gopher",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2368.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1288.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8520.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4373.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8777.4
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8190.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20051.4
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15718.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14840.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 105148.4
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15253.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34858.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 32487.2
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56218.2
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 104368.2
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17106.8
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10022.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8442.8
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 582.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33641.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Gravitar",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 429.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 387.7
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 306.7
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 538.4
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 473.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 298.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 588.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 412.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 297.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 200.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 167.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 548.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 269.5
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 483.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 238.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 320.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 303.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 269.5
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 238.68
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 498.3
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 238.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 805.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 440.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 HERO",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7295.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6459.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19950.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8963.4
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20437.8
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14992.9
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20818.2
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20130.2
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15207.9
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15459.2
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14892.5
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23037.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20889.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14225.2
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21036.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 32464.1
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28889.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28765.8
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 38874.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Ice Hockey",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -3.2
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -9.5
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.6
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.7
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.6
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.9
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.7
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.5
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.5
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1.3
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.2
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.4
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -1.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.7
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.1
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -3.5
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 James Bond",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 354.1
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 202.8
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 576.7
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 444.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 768.5
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 697.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1358.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1312.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 835.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 585.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 573.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5148.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3961.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 507.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 812.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 613.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 541.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 351.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1909.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Kangaroo",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8.8
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1622.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6740.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1431.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7259.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4496.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14854.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12992.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10334.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11204.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 861.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16200.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12185.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13150.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1792.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 125.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 106.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 94.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11200.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12853.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Krull",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3341.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3372.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3805.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6363.1
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8422.3
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6206.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11451.9
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8051.6
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7920.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7658.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6796.1
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9728.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6872.8
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9745.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10374.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8066.6
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5911.4
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5560.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8647.2
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9735.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Kung-Fu Master",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 29151.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19544.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23270.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20620.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26059.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20882.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34294.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 29710.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24288.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 37484.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 30207.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 39581.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31676.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34393.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 48375.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 40835.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28819.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3046.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 48192.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Montezuma's Revenge",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 259.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10.7
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 84.0
              }
            },
            {
              "model_name": "MP-EB",
              "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
              "paper_url": "https://arxiv.org/abs/1507.00814",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 142.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 47.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 42.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 51.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 67.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 53.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 41.0
              }
            },
            {
              "model_name": "DDQN-PC",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3459.0
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 273.7
              }
            },
            {
              "model_name": "TRPO-hash",
              "paper_title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.04717",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 75.0
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3705.5
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Sarsa-\u03c6-EB",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2745.4
              }
            },
            {
              "model_name": "Sarsa-\u03b5",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 399.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Ms. Pacman",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1227.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1692.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2311.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1263.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3085.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1092.3
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6283.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2711.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2250.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1241.3
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1007.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6518.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1865.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4963.8
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3327.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 850.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 653.7
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 594.4
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3415.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Name This Game",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2247.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2500.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7257.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9238.5
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8207.8
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6738.8
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11971.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11185.1
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10616.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13637.9
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8960.3
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12270.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10497.6
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15851.2
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15572.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12093.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10476.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5614.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4503.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12542.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Pong",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -17.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -19.0
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.9
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19.5
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20.9
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.8
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19.1
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.4
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20.6
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20.9
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11.4
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5.6
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20.9
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Private Eye",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 86.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 684.3
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1788.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2598.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 207.9
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 146.7
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 292.6
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 129.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 103.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1277.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -575.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 670.7
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 200.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 286.7
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 206.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 421.1
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 206.9
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 194.4
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 99.32
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8358.7
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 206.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 100.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15095.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Q*Bert",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 960.3
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 613.5
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4500
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10596.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7089.8
              }
            },
            {
              "model_name": "MP-EB",
              "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
              "paper_url": "https://arxiv.org/abs/1507.00814",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15805.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13117.3
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9271.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19220.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15088.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14175.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14063.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11020.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16256.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9944.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5236.8
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18760.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21307.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15148.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13752.3
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 147.5
              }
            },
            {
              "model_name": "Sarsa-\u03c6-EB",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4111.8
              }
            },
            {
              "model_name": "Sarsa-\u03b5",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3895.3
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23784.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 River Raid",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2650.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1904.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8316.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5310.3
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7377.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4748.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21162.6
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16569.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14884.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16496.8
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10838.4
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14522.3
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11807.2
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12530.8
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20607.6
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12201.8
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10001.2
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6591.9
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5009.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17322.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Road Runner",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 89.1
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 67.7
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18257.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 43079.8
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 39544.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 35215.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 69524.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 58549.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 44127.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 54630.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 43156.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 57608.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 52264.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 47770.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 62151.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 73949.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34216.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31769.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16590.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 55839.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Robotank",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28.7
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 51.6
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 61.8
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 63.9
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 58.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 65.3
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 65.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 62.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 59.1
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24.7
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 62.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56.2
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 64.3
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 27.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 32.8
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2.6
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2.3
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11.9
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 52.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Seaquest",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 675.5
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 664.8
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1740
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5286.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10145.9
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5860.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4216.7
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 50254.2
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 37361.6
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16452.7
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14498.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1431.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26357.8
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 25463.7
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10932.3
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 931.6
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2355.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2300.2
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1326.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1390.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 266434.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Space Invaders",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 267.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 250.1
              }
            },
            {
              "model_name": "DQN best",
              "paper_title": "Playing Atari with Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1312.5602",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1075
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1976.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1183.3
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1692.3
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1293.8
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6427.3
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5993.1
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2525.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8978.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2628.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3912.1
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2865.8
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2589.7
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15311.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23846.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15730.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2214.7
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 678.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5747.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Star Gunner",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1070.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 57997.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14919.2
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 54282.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 52970.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 90804.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 89238.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 60142.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 127073.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 58365.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 63302.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 61582.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 589.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 125117.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 164766.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 138218.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 64393.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1470.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 49095.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Tennis",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.1
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.5
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12.2
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11.1
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -22.8
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -7.8
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -5.3
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -6.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -6.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -10.2
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -4.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23.1
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Time Pilot",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3741.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5947.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8267.8
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4870.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4786.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11666.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8339.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6601.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6608.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4871.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9197.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5963.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4870.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7553.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 27202.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12679.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5825.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4970.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8329.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Tutankham",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 98.2
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 114.3
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 186.7
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 118.5
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 68.1
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 45.6
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 218.4
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 211.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 48.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 108.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 92.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 204.6
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 183.9
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 245.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 156.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 144.2
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 26.1
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 130.3
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 280.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Up and Down",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2449.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3533.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8456.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8747.7
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9989.9
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8038.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 44939.6
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24759.2
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22972.2
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22681.3
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19086.9
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16154.1
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12157.4
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22474.4
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33879.1
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 105728.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 74705.7
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 54525.4
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 67974.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15612.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Venture",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.6
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 66.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 380.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 523.4
              }
            },
            {
              "model_name": "MP-EB",
              "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
              "paper_url": "https://arxiv.org/abs/1507.00814",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 163.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 136.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 497.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 200.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 98.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 29.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 94.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 54.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1172.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 48.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 25.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19.0
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "TRPO-hash",
              "paper_title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.04717",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 445.0
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 82.2
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 48.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 760.0
              }
            },
            {
              "model_name": "Sarsa-\u03c6-EB",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1169.2
              }
            },
            {
              "model_name": "Sarsa-\u03b5",
              "paper_title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1706.08090",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1520.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Video Pinball",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 19761.0
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16871.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 42684.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 112093.4
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 196760.4
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 154414.1
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 309941.9
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 110976.2
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 98209.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 447408.6
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 367823.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 295972.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 282007.3
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56287.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 479197.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 470310.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 331628.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 185852.6
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 22834.8
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 949604.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Wizard of Wor",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 36.9
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1981.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3393.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10431.0
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2704.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1609.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7855.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7492.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7054.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10471.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6201.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5727.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4802.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 483.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12352.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18082.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17244.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5278.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3480.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9300.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Zaxxon",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "SARSA",
              "paper_title": "Investigating Contingency Awareness Using Atari 2600 Games",
              "paper_url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5162",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21.4
              }
            },
            {
              "model_name": "Best linear",
              "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
              "paper_url": "https://arxiv.org/abs/1207.4708v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3365.0
              }
            },
            {
              "model_name": "Nature DQN",
              "paper_title": "Human-level control through deep reinforcement learning",
              "paper_url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4977.0
              }
            },
            {
              "model_name": "Gorila",
              "paper_title": "Massively Parallel Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1507.04296",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6159.4
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5363.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4412.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12944.0
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10164.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10163.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11320.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8593.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10469.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 9474.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 14402.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 13886.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 24622.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23519.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2659.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6380.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 10513.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Phoenix",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8485.2
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7484.8
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23092.2
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 20410.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12252.5
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 63597.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 12366.5
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18992.7
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16903.6
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6202.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 70324.3
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 74786.7
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 52894.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 28181.8
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4041.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 17490.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Pitfall!",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -113.2
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -286.1
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -29.9
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -46.9
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -186.7
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -243.6
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -356.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -427.0
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.6
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -78.5
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -123.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -135.7
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -259.09
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 0.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Skiing",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -12142.1
              }
            },
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13062.3
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -8857.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -9021.8
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -11928.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -11490.4
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -18955.8
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -9996.9
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -10169.1
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13585.1
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -19949.9
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -10911.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13700.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -14863.8
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -15442.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -13901.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Solaris",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3482.8
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1295.4
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 3067.8
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2250.8
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1768.4
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 810.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 280.6
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4309.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2272.8
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4544.8
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 133.4
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1956.0
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1936.4
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1884.8
              }
            },
            {
              "model_name": "A3C-CTS",
              "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
              "paper_url": "https://arxiv.org/abs/1606.01868",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2270.15
              }
            },
            {
              "model_name": "DQN-PixelCNN",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2863.6
              }
            },
            {
              "model_name": "DQN-CTS",
              "paper_title": "Count-Based Exploration with Neural Density Models",
              "paper_url": "https://arxiv.org/abs/1703.01310v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 133.4
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 2090.0
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8342.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Yars Revenge",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 18098.9
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4577.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 49622.1
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 25976.5
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11712.6
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 58145.9
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6270.6
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11357.0
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4687.4
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 21409.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 69618.1
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7270.8
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 7157.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5615.5
              }
            },
            {
              "model_name": "ES FF (1 hour) noop",
              "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1703.03864v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 16401.7
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 35050.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Defender",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23633.0
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 15917.5
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 42214.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 35338.5
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 33996.0
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 34415.0
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 27510.0
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 31286.5
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 23666.5
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 11099.0
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 41324.5
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 233021.5
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 56533.0
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 36242.5
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 47092.0
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Atari 2600 Surround",
        "description": "",
        "sota": {
          "metrics": [
            "Score"
          ],
          "rows": [
            {
              "model_name": "DQN noop",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -5.6
              }
            },
            {
              "model_name": "DQN hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -6.0
              }
            },
            {
              "model_name": "Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4.4
              }
            },
            {
              "model_name": "Duel hs",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 4.0
              }
            },
            {
              "model_name": "DDQN (tuned) noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.9
              }
            },
            {
              "model_name": "DDQN (tuned) hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1.9
              }
            },
            {
              "model_name": "Prior+Duel hs",
              "paper_title": "Deep Reinforcement Learning with Double Q-learning",
              "paper_url": "https://arxiv.org/abs/1509.06461v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -0.2
              }
            },
            {
              "model_name": "Prior noop",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 8.9
              }
            },
            {
              "model_name": "Prior hs",
              "paper_title": "Prioritized Experience Replay",
              "paper_url": "https://arxiv.org/abs/1511.05952",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 5.9
              }
            },
            {
              "model_name": "DDQN+Pop-Art noop",
              "paper_title": "Learning functions across many orders of magnitudes",
              "paper_url": "https://arxiv.org/abs/1602.07714v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -2.5
              }
            },
            {
              "model_name": "Prior+Duel noop",
              "paper_title": "Dueling Network Architectures for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1511.06581v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 1.2
              }
            },
            {
              "model_name": "A3C LSTM hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -8.3
              }
            },
            {
              "model_name": "A3C FF (1 day) hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -9.6
              }
            },
            {
              "model_name": "A3C FF hs",
              "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1602.01783",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": -9.7
              }
            },
            {
              "model_name": "C51 noop",
              "paper_title": "A Distributional Perspective on Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1707.06887v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Score": 6.8
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Abstract Games with Hints",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Computer Chess",
        "description": "",
        "sota": {
          "metrics": [
            "ELO rating"
          ],
          "rows": [
            {
              "model_name": "Novag Super Constellation 6502 4 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 1631
              }
            },
            {
              "model_name": "Mephisto Amsterdam 68000 12 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 1827
              }
            },
            {
              "model_name": "Mephisto Amsterdam 68000 12 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 1827
              }
            },
            {
              "model_name": "Mephisto Dallas 68020 14 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 1923
              }
            },
            {
              "model_name": "Mephisto MM 4 Turbo Kit 6502 16 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 1993
              }
            },
            {
              "model_name": "Mephisto Portorose 68020 12 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2027
              }
            },
            {
              "model_name": "Mephisto Portorose 68030 36 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2138
              }
            },
            {
              "model_name": "Mephisto Vancouver 68030 36 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2127
              }
            },
            {
              "model_name": "Chess Machine Schroder 3.0 ARM2 30 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2174
              }
            },
            {
              "model_name": "Mephisto Genius 2.0 486/50-66 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2235
              }
            },
            {
              "model_name": "MChess Pro 5.0 Pentium 90 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2306
              }
            },
            {
              "model_name": "Rebel 8.0 Pentium 90 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2337
              }
            },
            {
              "model_name": "Deep Blue",
              "paper_title": "What was Deep Blue's Elo rating? - Quora",
              "paper_url": "https://www.quora.com/What-was-Deep-Blues-Elo-rating",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2725
              }
            },
            {
              "model_name": "HIARCS 6.0 49MB P200 MMX",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2418
              }
            },
            {
              "model_name": "Fritz 5.0 PB29% 67MB P200 MMX",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2460
              }
            },
            {
              "model_name": "Chess Tiger 12.0 DOS 128MB K6-2 450 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2594
              }
            },
            {
              "model_name": "Fritz 6.0 128MB K6-2 450 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2607
              }
            },
            {
              "model_name": "Chess Tiger 14.0 CB 256MB Athlon 1200",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2709
              }
            },
            {
              "model_name": "Deep Fritz 7.0 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2759
              }
            },
            {
              "model_name": "Shredder 7.04 UCI 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2791
              }
            },
            {
              "model_name": "Shredder 8.0 CB 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2800
              }
            },
            {
              "model_name": "Shredder 9.0 UCI 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2808
              }
            },
            {
              "model_name": "Rybka 1.1 64bit",
              "paper_title": "CCRL 40/40 - Complete list",
              "paper_url": "https://web.archive.org/web/20060531091049/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2995
              }
            },
            {
              "model_name": "Rybka 1.2 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2902
              }
            },
            {
              "model_name": "Rybka 2.3.1 Arena 256MB Athlon 1200 MHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 2935
              }
            },
            {
              "model_name": "Deep Rybka 3 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3238
              }
            },
            {
              "model_name": "Deep Rybka 3 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3232
              }
            },
            {
              "model_name": "Rybka 4 64bit",
              "paper_title": "CCRL 40/40 - Complete list",
              "paper_url": "https://web.archive.org/web/20100923131123/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3269
              }
            },
            {
              "model_name": "Deep Rybka 3 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3227
              }
            },
            {
              "model_name": "Deep Rybka 4 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3216
              }
            },
            {
              "model_name": "Deep Rybka 4 x64 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3221
              }
            },
            {
              "model_name": "Houdini 3 64bit",
              "paper_title": "Wayback Machine",
              "paper_url": "https://web.archive.org/web/20130415000000*/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3248
              }
            },
            {
              "model_name": "Komodo 5.1 MP x64 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3241
              }
            },
            {
              "model_name": "Komodo 7.0 MP x64 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3295
              }
            },
            {
              "model_name": "Komodo 9",
              "paper_title": "CCRL 40/40 - Complete list",
              "paper_url": "https://web.archive.org/web/20150708104805/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3332
              }
            },
            {
              "model_name": "Stockfish 6 MP x64 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3334
              }
            },
            {
              "model_name": "Komodo 9.1 MP x64 2GB Q6600 2.4 GHz",
              "paper_title": "Swedish Chess Computer Association - Wikipedia",
              "paper_url": "https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3366
              }
            },
            {
              "model_name": "Stockfish",
              "paper_title": "CCRL 40/40 - Index",
              "paper_url": "https://web.archive.org/web/20170227044521/http://www.computerchess.org.uk/ccrl/4040/",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "ELO rating": 3393
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Language Modelling",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "Penn Treebank (Perplexity when parsing English sentences)",
        "description": "",
        "sota": {
          "metrics": [
            "Perplexity"
          ],
          "rows": [
            {
              "model_name": "KN5+RNNME ensemble",
              "paper_title": null,
              "paper_url": "http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 78.8
              }
            },
            {
              "model_name": "KN5+cache baseline",
              "paper_title": null,
              "paper_url": "http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 125.7
              }
            },
            {
              "model_name": "RNN-LDA+all",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 74.1
              }
            },
            {
              "model_name": "RNN-LDA ensemble",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 80.1
              }
            },
            {
              "model_name": "RNN-LDA LM+KN5+cache",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 92.0
              }
            },
            {
              "model_name": "RNN-LDA LM",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 113.7
              }
            },
            {
              "model_name": "RNNLM",
              "paper_title": null,
              "paper_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 124.7
              }
            },
            {
              "model_name": "Deep RNN",
              "paper_title": "How to Construct Deep Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1312.6026",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 107.5
              }
            },
            {
              "model_name": "RNN Dropout Regularization",
              "paper_title": "Recurrent Neural Network Regularization",
              "paper_url": "https://arxiv.org/abs/1409.2329v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 68.7
              }
            },
            {
              "model_name": "RHN",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 71.3
              }
            },
            {
              "model_name": "Pointer Sentinel-LSTM",
              "paper_title": "Pointer Sentinel Mixture Models",
              "paper_url": "https://arxiv.org/abs/1609.07843v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 70.9
              }
            },
            {
              "model_name": "Variational LSTM",
              "paper_title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1512.05287v5",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 73.4
              }
            },
            {
              "model_name": "RHN+WT",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 66
              }
            },
            {
              "model_name": "RHN",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 68.5
              }
            },
            {
              "model_name": "Neural Architecture Search",
              "paper_title": "Neural Architecture Search with Reinforcement Learning",
              "paper_url": "https://arxiv.org/abs/1611.01578v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 62.4
              }
            },
            {
              "model_name": "RHN+WT",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Perplexity": 65.4
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Hutter Prize (bits per character to encode English text)",
        "description": "",
        "sota": {
          "metrics": [
            "Model Entropy"
          ],
          "rows": [
            {
              "model_name": "RNN",
              "paper_title": null,
              "paper_url": "http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.6
              }
            },
            {
              "model_name": "RNN, LSTM",
              "paper_title": "Generating Sequences With Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1308.0850",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.67
              }
            },
            {
              "model_name": "Gated Feedback RNN",
              "paper_title": "Gated Feedback Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1502.02367",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.58
              }
            },
            {
              "model_name": "Grid LSTM",
              "paper_title": "Grid Long Short-Term Memory",
              "paper_url": "https://arxiv.org/abs/1507.01526",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.47
              }
            },
            {
              "model_name": "Recurrent Highway Networks",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.32
              }
            },
            {
              "model_name": "RHN",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.42
              }
            },
            {
              "model_name": " Hierarchical Multiscale RNN",
              "paper_title": "Hierarchical Multiscale Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1609.01704",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.32
              }
            },
            {
              "model_name": "Hypernetworks",
              "paper_title": "HyperNetworks",
              "paper_url": "https://arxiv.org/abs/1609.09106",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.39
              }
            },
            {
              "model_name": "Surprisal-Driven Feedback RNN",
              "paper_title": "Surprisal-Driven Feedback in Recurrent Networks",
              "paper_url": "https://arxiv.org/abs/1608.06027",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.37
              }
            },
            {
              "model_name": "Surprisal-Driven Zoneout",
              "paper_title": null,
              "paper_url": "https://pdfs.semanticscholar.org/e9bc/83f9ff502bec9cffb750468f76fdfcf5dd05.pdf",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.313
              }
            },
            {
              "model_name": "ByteNet",
              "paper_title": "Neural Machine Translation in Linear Time",
              "paper_url": "https://arxiv.org/abs/1610.10099",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.31
              }
            },
            {
              "model_name": "Large RHN depth 10",
              "paper_title": "Recurrent Highway Networks",
              "paper_url": "https://arxiv.org/abs/1607.03474v4",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Model Entropy": 1.27
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "LAMBADA prediction of words in discourse",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "GA + feat.",
              "paper_title": "Broad Context Language Modeling as Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1610.08431v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 49.0
              }
            },
            {
              "model_name": "AS + feat.",
              "paper_title": "Broad Context Language Modeling as Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1610.08431v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 44.5
              }
            },
            {
              "model_name": "Modified Stanford",
              "paper_title": "Broad Context Language Modeling as Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1610.08431v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 32.1
              }
            },
            {
              "model_name": "Stanford Reader",
              "paper_title": "Broad Context Language Modeling as Reading Comprehension",
              "paper_url": "https://arxiv.org/abs/1610.08431v3",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 21.7
              }
            },
            {
              "model_name": "GA+MAGE (48)",
              "paper_title": "Linguistic Knowledge as Memory for Recurrent Neural Networks",
              "paper_url": "https://arxiv.org/abs/1703.02620v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 51.6
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Language Creation Games",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Adversarial Defense",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Conditional Expression Parsing",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Visual Question Answering",
    "description": "",
    "categories": [],
    "datasets": [
      {
        "dataset": "COCO Visual Question Answering (VQA) real images 1.0 open ended",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "LSTM Q+I",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1505.00468v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 58.2
              }
            },
            {
              "model_name": "iBOWIMG baseline",
              "paper_title": "Simple Baseline for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1512.02167",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 55.89
              }
            },
            {
              "model_name": "SAN",
              "paper_title": "Stacked Attention Networks for Image Question Answering",
              "paper_url": "https://arxiv.org/abs/1511.02274v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 58.9
              }
            },
            {
              "model_name": "CNN-RNN",
              "paper_title": "Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge",
              "paper_url": "https://arxiv.org/abs/1603.02814v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 59.5
              }
            },
            {
              "model_name": "SMem-VQA",
              "paper_title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1511.05234v2",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 58.24
              }
            },
            {
              "model_name": "FDA",
              "paper_title": "A Focused Dynamic Attention Model for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1604.01485v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 59.5
              }
            },
            {
              "model_name": "HQI+ResNet",
              "paper_title": "Hierarchical Co-Attention for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.00061v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 62.1
              }
            },
            {
              "model_name": "MRN + global features",
              "paper_title": "Multimodal Residual Learning for Visual QA",
              "paper_url": "https://arxiv.org/abs/1606.01455v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.84
              }
            },
            {
              "model_name": "MCB 7 att.",
              "paper_title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
              "paper_url": "https://arxiv.org/abs/1606.01847v1",
              "paper_date": null,
              "code_links": [
                {
                  "title": "Replicated",
                  "url": "https://github.com/akirafukui/vqa-mcb"
                }
              ],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.5
              }
            },
            {
              "model_name": "joint-loss",
              "paper_title": "Training Recurrent Answering Units with Joint Loss Minimization for VQA",
              "paper_url": "https://arxiv.org/abs/1606.03647",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.2
              }
            },
            {
              "model_name": "N2NMN",
              "paper_title": "Learning to Reason: End-to-End Module Networks for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1704.05526v2",
              "paper_date": null,
              "code_links": [
                {
                  "title": "Replicated",
                  "url": "http://ronghanghu.com/n2nmn/"
                }
              ],
              "model_links": [],
              "metrics": {
                "Percentage correct": 64.2
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "COCO Visual Question Answering (VQA) real images 1.0 multiple choice",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "LSTM Q+I",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1505.00468v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 63.1
              }
            },
            {
              "model_name": "iBOWIMG baseline",
              "paper_title": "Simple Baseline for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1512.02167",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.97
              }
            },
            {
              "model_name": "FDA",
              "paper_title": "A Focused Dynamic Attention Model for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1604.01485v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 64.2
              }
            },
            {
              "model_name": "HQI+ResNet",
              "paper_title": "Hierarchical Co-Attention for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1606.00061v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.1
              }
            },
            {
              "model_name": "MRN",
              "paper_title": "Multimodal Residual Learning for Visual QA",
              "paper_url": "https://arxiv.org/abs/1606.01455v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 66.33
              }
            },
            {
              "model_name": "MCB 7 att.",
              "paper_title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
              "paper_url": "https://arxiv.org/abs/1606.01847v1",
              "paper_date": null,
              "code_links": [
                {
                  "title": "Replicated",
                  "url": "https://github.com/akirafukui/vqa-mcb"
                }
              ],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.1
              }
            },
            {
              "model_name": "joint-loss",
              "paper_title": "Training Recurrent Answering Units with Joint Loss Minimization for VQA",
              "paper_url": "https://arxiv.org/abs/1606.03647",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 67.3
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "COCO Visual Question Answering (VQA) abstract images 1.0 open ended",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Dualnet ensemble",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/aoe.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.73
              }
            },
            {
              "model_name": "LSTM + global features",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/aoe.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 65.02
              }
            },
            {
              "model_name": "LSTM blind",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/aoe.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 57.19
              }
            },
            {
              "model_name": "Graph VQA",
              "paper_title": "Graph-Structured Representations for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1609.05600v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.42
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "COCO Visual Question Answering (VQA) abstract 1.0 multiple choice",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "Dualnet ensemble",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/amc.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 71.18
              }
            },
            {
              "model_name": "LSTM + global features",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/amc.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 69.21
              }
            },
            {
              "model_name": "LSTM blind",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://visualqa.org/amc.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 61.41
              }
            },
            {
              "model_name": "Graph VQA",
              "paper_title": "Graph-Structured Representations for Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1609.05600v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 74.37
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Visual Genome (pairs)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "CMN",
              "paper_title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks",
              "paper_url": "https://arxiv.org/abs/1611.09978v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 28.52
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Visual Genome (subjects)",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "CMN",
              "paper_title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks",
              "paper_url": "https://arxiv.org/abs/1611.09978v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 44.24
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "Visual7W",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "MCB+Att.",
              "paper_title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
              "paper_url": "https://arxiv.org/abs/1606.01847v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 62.2
              }
            },
            {
              "model_name": "CMN",
              "paper_title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks",
              "paper_url": "https://arxiv.org/abs/1611.09978v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 72.53
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      },
      {
        "dataset": "COCO Visual Question Answering (VQA) real images 2.0 open ended",
        "description": "",
        "sota": {
          "metrics": [
            "Percentage correct"
          ],
          "rows": [
            {
              "model_name": "MCB",
              "paper_title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1612.00837v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 62.27
              }
            },
            {
              "model_name": "d-LSTM+nI",
              "paper_title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
              "paper_url": "https://arxiv.org/abs/1612.00837v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 54.22
              }
            },
            {
              "model_name": "Up-Down",
              "paper_title": "Bottom-Up and Top-Down Attention for Image Captioning and VQA",
              "paper_url": "https://arxiv.org/abs/1707.07998v1",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 70.34
              }
            },
            {
              "model_name": "HDU-USYD-UNCC",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://www.visualqa.org/roe_2017.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.16
              }
            },
            {
              "model_name": "DLAIT",
              "paper_title": "VQA: Visual Question Answering",
              "paper_url": "http://www.visualqa.org/roe_2017.html",
              "paper_date": null,
              "code_links": [],
              "model_links": [],
              "metrics": {
                "Percentage correct": 68.07
              }
            }
          ]
        },
        "dataset_links": [],
        "dataset_citations": []
      }
    ],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Complex Conditional Image Generation",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Underconstrained Problem Solving",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  },
  {
    "task": "Side Effect Mitigation",
    "description": "",
    "categories": [],
    "datasets": [],
    "subtasks": [],
    "synonyms": [],
    "source_link": {
      "title": "Progress of AI Research",
      "url": "https://github.com/AI-metrics/AI-metrics"
    }
  }
]